{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use \"other_layer\" since it corrects some mistakes from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries and defining the Net class (same class with which the networks were generated)\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import quadprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "k880Ih2e3PBf"
   },
   "outputs": [],
   "source": [
    "# Network class\n",
    "k=1\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(3, 16*k,  kernel_size=3) , nn.BatchNorm2d(16*k),\n",
    "                      nn.ReLU(inplace=True)), nn.Sequential(nn.Conv2d(16*k, 16*k,  kernel_size=3) , nn.BatchNorm2d(16*k),\n",
    "                      nn.ReLU(inplace=True))] \n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(16*k, 32*k,  kernel_size=3, stride=2),  nn.BatchNorm2d(32*k),\n",
    "                      nn.ReLU(inplace=True))]\n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(32*k, 32*k,  kernel_size=3),  nn.BatchNorm2d(32*k),\n",
    "              nn.ReLU(inplace=True)),nn.Sequential(nn.Conv2d(32*k, 32*k,  kernel_size=3),  nn.BatchNorm2d(32*k),\n",
    "              nn.ReLU(inplace=True))]\n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(32*k, 64*k,  kernel_size=3, stride=2), nn.BatchNorm2d(64*k),\n",
    "                      nn.ReLU(inplace=True))]\n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(64*k, 64*k,  kernel_size=3, padding='valid'), nn.BatchNorm2d(64*k),\n",
    "                      nn.ReLU(inplace=True))]\n",
    "        self.layers+=[nn.Sequential(nn.Conv2d(64*k, 64*k,  kernel_size=1), nn.BatchNorm2d(64*k),\n",
    "                      nn.ReLU(inplace=True))]\n",
    "        self.layers+= [nn.AdaptiveAvgPool2d((1,1))]\n",
    "        self.fc = nn.Linear(64*k, 10)\n",
    "    \n",
    "    def forward(self, x, acts_only=False,all_act=False):\n",
    "        all_acts = []\n",
    "        for i in range(len(self.layers)):\n",
    "            all_acts.append(x)\n",
    "            x = self.layers[i](x)\n",
    "\n",
    "        x = self.fc(x.view(-1, 64*k))\n",
    "\n",
    "        if all_act:\n",
    "            # all_cts does not return the final output of the network\n",
    "            return all_acts, x\n",
    "        return x\n",
    "    \n",
    "#     def forward_embed(self, x, layer_idx = -1):\n",
    "#         if layer_idx != -1:\n",
    "#             for i in range(len(self.layers[:layer_idx])): # layer index starts at 0\n",
    "#                 x = self.layers[i](x)\n",
    "#         else:\n",
    "#             for i in range(len(self.layers)):\n",
    "#                 x = self.layers[i](x)\n",
    "                \n",
    "#         return x\n",
    "    def forward_embed(self, x, layer_idx = -1):\n",
    "        for i in range(len(self.layers[:layer_idx])):\n",
    "             x = self.layers[i](x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "# Defining linear CKA\n",
    "class LinCKA(nn.Module):\n",
    "    def __init__(self, n=1000):\n",
    "        super(LinCKA, self).__init__()\n",
    "        self.resetK(n)\n",
    "    \n",
    "    def resetK(self,n):\n",
    "        unit = torch.ones([n, n])\n",
    "        I = torch.eye(n)\n",
    "        H = I - unit / n\n",
    "        H = H.cuda()\n",
    "        self.H = H.cuda()\n",
    "        self.n = n\n",
    "\n",
    "    def centering(self, K):\n",
    "        H = self.H\n",
    "        return torch.matmul(torch.matmul(H, K), H) \n",
    "\n",
    "    def linear_HSIC(self, X, Y):\n",
    "        L_X = torch.matmul(X, X.T)\n",
    "        L_Y = torch.matmul(Y, Y.T)\n",
    "        return torch.sum(self.centering(L_X) * self.centering(L_Y))\n",
    "\n",
    "    def linear_CKA(self,X, Y):\n",
    "        hsic = self.linear_HSIC(X, Y)\n",
    "        var1 = torch.sqrt(self.linear_HSIC(X, X))\n",
    "        var2 = torch.sqrt(self.linear_HSIC(Y, Y))\n",
    "\n",
    "        return hsic / (var1 * var2),var1,var2\n",
    "\n",
    "    def forward(self, X,Y):\n",
    "        if len(X) != self.n:\n",
    "            self.resetK(len(X))\n",
    "        return self.linear_CKA(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "522e0f024813495bb45905626fdccd14",
      "916a6726fe5b4b8a949d1d6b1d4bfec1",
      "8de220001a4c42e6b2bf369d63e17658",
      "9a5b5204bd3847d485f8a139949a79f4",
      "b0283df5d40143789293e11c9428d543",
      "ad439d60a52447469bcdce66a8e3c0bd",
      "6d746b9790c744ac91ff6c7cc852f9f7",
      "890787b726f4485199dcb2a059ef92a3",
      "e34af8bd1fe3408cbd99341d4271f845",
      "8ec5fa1d3675438ba3e5097f6f605325",
      "70d14a62a0e94362bea840e44b5719e3"
     ]
    },
    "id": "Fzk7lZmit4K4",
    "outputId": "1852d041-28c8-499d-c2bb-7049903c0fb4"
   },
   "outputs": [],
   "source": [
    "# # Import data \n",
    "# from torchvision import datasets, transforms\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                   std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# transform_val = transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "# transform_train =  transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# ##### Cifar Data\n",
    "# cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "# cifar_data_test = datasets.CIFAR10(root='.',train=False, transform=transform_val, download=True)\n",
    "# n=10000\n",
    "# val_loaderx = torch.utils.data.DataLoader(cifar_data_test,\n",
    "#                                            batch_size=n, \n",
    "#                                            shuffle=False)\n",
    "# data, labels = iter(val_loaderx).next()\n",
    "# data = data.to(device)\n",
    "\n",
    "# train_loaderx = torch.utils.data.DataLoader(cifar_data,\n",
    "#                                            batch_size=50000, \n",
    "#                                            shuffle=False)\n",
    "# train_data, train_labels = iter(train_loaderx).next()\n",
    "# train_data = train_data.to(device)\n",
    "# train_labels = train_labels.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxrHSBk_Yhrq",
    "outputId": "45088feb-7b91-4ec4-e3e1-348e8228f309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the saved networks\n",
    "PATH='net_kornblith/net_kornblith_'\n",
    "\n",
    "# net_rand1 = Net()\n",
    "# net_rand1.load_state_dict(torch.load(PATH+'random_1.zip'))\n",
    "# net_rand1.eval()\n",
    "# net_rand1.cuda()\n",
    "\n",
    "# net_rand2 = Net()\n",
    "# net_rand1.load_state_dict(torch.load(PATH+'random_2.zip'))\n",
    "# net_rand2.eval()\n",
    "# net_rand2.cuda()\n",
    "\n",
    "net_all1 = Net()\n",
    "net_all1.load_state_dict(torch.load(PATH+'all_1.zip'))\n",
    "net_all1.eval()\n",
    "net_all1.cuda()\n",
    "\n",
    "# net_all2 = Net()\n",
    "# net_all2.load_state_dict(torch.load(PATH+'all_2.zip'))\n",
    "# net_all2.eval()\n",
    "# net_all2.cuda()\n",
    "\n",
    "# net_mem_s1 = Net()\n",
    "# net_mem_s1.load_state_dict(torch.load(PATH+'mem_1_set1.zip'))\n",
    "# net_mem_s1.eval()\n",
    "# net_mem_s1.cuda()\n",
    "\n",
    "# net_mem_s2 = Net()\n",
    "# net_mem_s2.load_state_dict(torch.load(PATH+'mem_2_set2.zip'))\n",
    "# net_mem_s2.eval()\n",
    "# net_mem_s2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and train experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_by_class(data, labels):\n",
    "    indexes = []\n",
    "    datapoints = []\n",
    "    \n",
    "    for label in range(10): #for label in set(labels): \n",
    "        indexes.append(np.where(labels==label))\n",
    "        datapoints.append(data[indexes[-1]])\n",
    "    \n",
    "    return datapoints, np.array(indexes).squeeze()\n",
    "\n",
    "def split_class_clusters(data, indexes, d, W, n_clusters, split_constant, ortho_d = True, experiment = 'one_class_one_point'):\n",
    "    # data should be of shape n x d (n examples, d features)\n",
    "    # d is the random direction vector\n",
    "    # W comes from lin_svc.coef_ (n_classes, n_features) corresponding to the normal vectors to the decision hyperplanes\n",
    "    data_ = np.copy(data)\n",
    "    \n",
    "    if ortho_d:\n",
    "        Q, R = np.linalg.qr(W.T)\n",
    "        d_ = d.reshape([1,np.max(d.shape)])\n",
    "        d_ -= np.matmul(d_, np.matmul(Q, Q.T))\n",
    "#     print(\"Direction after removing components: {}\".format(d_))\n",
    "    else:\n",
    "        d_ = d\n",
    "    \n",
    "    projections = np.matmul(data_, d_.T).squeeze()\n",
    "    mean = np.mean(projections)\n",
    "    min_ = np.min(projections)\n",
    "    max_ = np.max(projections)\n",
    "    \n",
    "    translations = np.zeros(data_.shape[0])\n",
    "    \n",
    "    idxs = []\n",
    "    separators = np.linspace(min_,max_,n_clusters+1)\n",
    "    if 'one_point' in experiment: \n",
    "        if n_clusters != 2:\n",
    "            print(\"ERROR: one point experiment won't work because number of clusters is not 2\")\n",
    "        else:\n",
    "            separators = np.linspace(min_,max_,n_clusters)\n",
    "        \n",
    "    for cluster_idx in range(n_clusters):\n",
    "        idx = np.where(projections>=separators[cluster_idx])\n",
    "        if cluster_idx < (n_clusters-1): # Except for last cluster we need to take into account that the projections are not superior to the next separator\n",
    "            idx2 = idx2 = np.where(projections<separators[cluster_idx+1])\n",
    "            idx = np.intersect1d(idx, idx2)\n",
    "        idxs.append(idx)\n",
    "        translations[idxs[-1]] = cluster_idx*split_constant \n",
    "#     print(\"Translations before mult: {}\".format(translations))\n",
    "    \n",
    "    translations = np.matmul(np.diag(translations.squeeze()), np.matmul(np.ones([data_.shape[0],1]), d_))\n",
    "#     print(\"Translations after mult: {}\".format(translations))\n",
    "    \n",
    "    data_+= translations\n",
    "    return data_, np.array([indexes[i] for i in idxs]).squeeze()\n",
    "\n",
    "def test_cka_lin_sep(data_per_classes, indexes, lin_svc,\n",
    "                     num_clusters = 2,\n",
    "                     dist_clusters = 100,\n",
    "                     splitting_dir='pc4',\n",
    "                     num_pts_cka = 10000,\n",
    "                     seed = 0,\n",
    "                     experiment = \"one_class_one_point\"):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    print('Number of clusters: {}; Distance between clusters: {}; Splitting direction: '.format(num_clusters, dist_clusters)+splitting_dir+'; Number of points to compute CKA: {}'.format(num_pts_cka))\n",
    "    \n",
    "    labels = np.zeros(len(data_per_classes[0]))\n",
    "    split_data = []\n",
    "    split_indexes = []\n",
    "    \n",
    "    if splitting_dir == 'num_solve':\n",
    "        print('Numerically find a distance for all classes')\n",
    "        Q, R = np.linalg.qr(lin_svc.coef_.T)\n",
    "            \n",
    "        M = Q.T\n",
    "        # P = np.dot(M.T, M) # Not positive definite\n",
    "        epsilon = 1e-7\n",
    "        P = np.dot(M.T, M) + epsilon*np.eye(M.shape[1]) # Adding epsilon * identity to make it positive definite\n",
    "        q = -np.dot(M.T, np.zeros(10))\n",
    "        G = -np.eye(256)\n",
    "        # h = np.zeros(256) # returns all zeros\n",
    "        h =  -np.ones(256)*0.1\n",
    "            \n",
    "        direction = quadprog_solve_qp(P, q, G, h)\n",
    "        norm = np.sum(direction**2)**(0.5)\n",
    "        direction = direction/norm\n",
    "    \n",
    "    # Iterate the splitting for each class\n",
    "    for class_idx, class_data in enumerate(data_per_classes):\n",
    "        \n",
    "        # Define the direction along which to split the data\n",
    "        if 'pc' in splitting_dir:\n",
    "            direction = PCA(n_components = int(splitting_dir[2:])).fit(class_data).components_[int(splitting_dir[2:])-1]\n",
    "        if splitting_dir == 'random':\n",
    "#             # Sample from un-centered cube (not to be used)\n",
    "#             direction = np.random.rand(1, class_data.shape[1])\n",
    "#             # Sample from sphere using cubes (takes way to much time)\n",
    "#             while np.linalg.norm(direction)> 1:\n",
    "#                 direction = np.random.rand(1, class_data.shape[1])\n",
    "#                 direction = direction/np.linalg.norm(direction)\n",
    "            direction = np.random.normal(0, 1, class_data.shape[1])\n",
    "            norm = np.sum(direction**2)**(0.5)\n",
    "            direction = direction/norm\n",
    "            direction = np.absolute(direction)\n",
    "#             print(\"Direction: {}\".format(direction))\n",
    "        \n",
    "        # Split the data\n",
    "        if 'one_class' in experiment:\n",
    "            if class_idx > 0: dist_clusters = 0\n",
    "                \n",
    "        splits = split_class_clusters(class_data, indexes[class_idx], direction, lin_svc.coef_, num_clusters, dist_clusters, experiment = experiment)\n",
    "        split_data.append(splits[0])\n",
    "        split_indexes.append([i for i in splits[1]])\n",
    "        if class_idx != 0: labels = np.concatenate([labels, class_idx*np.ones(split_data[-1].shape[0])])\n",
    "        \n",
    "    split_data = np.concatenate(split_data)\n",
    "    original_data = torch.Tensor(np.concatenate(data_per_classes)).cuda()\n",
    "    \n",
    "    lin_sep = lin_svc.score(split_data, labels)\n",
    "    print(\"Accuracy of the linear SVM classifier on the split data: {}\".format(lin_sep))\n",
    "    \n",
    "    split_data = torch.Tensor(split_data).cuda()\n",
    "    # CKA values\n",
    "    CKA = LinCKA()\n",
    "    perm = np.random.permutation(num_pts_cka)\n",
    "    cka = CKA(original_data[perm], split_data[perm])[0].item()\n",
    "    print(\"Cka between {} original vs split pts: {}\".format(num_pts_cka, cka))\n",
    "    return lin_sep, cka, split_data.cpu().numpy(), np.concatenate(indexes), split_indexes\n",
    "\n",
    "def quadprog_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    qp_G = .5 * (P + P.T)   # make sure P is symmetric\n",
    "    qp_a = -q\n",
    "    if A is not None:\n",
    "        qp_C = -numpy.vstack([A, G]).T\n",
    "        qp_b = -numpy.hstack([b, h])\n",
    "        meq = A.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = -G.T\n",
    "        qp_b = -h\n",
    "        meq = 0\n",
    "    return quadprog.solve_qp(qp_G, qp_a, qp_C, qp_b, meq)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save the data (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_mod(datasets.CIFAR10):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        a = super().__getitem__(index)\n",
    "        return (a[0], a[1], index, torch.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "##### Loading CIFAR10 data\n",
    "cifar_data = CIFAR10_mod(root='.',train=True, transform=transform, download=True)\n",
    "\n",
    "#### Creading data loaders\n",
    "train_loaderx = torch.utils.data.DataLoader(cifar_data, batch_size=50000, shuffle=False)\n",
    "\n",
    "train_data, train_labels, train_indexes, train_embeds = iter(train_loaderx).next()\n",
    "train_data = train_data.to(device)\n",
    "train_labels = train_labels.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a linear SVM classifier on the original data: 0.9117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "train_activations, _ = net_all1.forward(train_data, all_act=True)\n",
    "train_act = train_activations[-1].reshape(train_activations[-1].shape[0],-1).detach().cpu().numpy()\n",
    "train_act_tensors = train_activations[-1]\n",
    "for i in train_activations[:-1]: del i\n",
    "del train_activations\n",
    "\n",
    "# Linear separability:\n",
    "lin_svc = LinearSVC()\n",
    "lin_svc.fit(train_act, train_labels)\n",
    "original_lin_sep = lin_svc.score(train_act, train_labels)\n",
    "print(\"Accuracy of a linear SVM classifier on the original data: {}\".format(original_lin_sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2; Distance between clusters: 2000; Splitting direction: num_solve; Number of points to compute CKA: 10000\n",
      "Numerically find a distance for all classes\n",
      "Accuracy of the linear SVM classifier on the split data: 0.9117\n",
      "Cka between 10000 original vs split pts: 0.23802901804447174\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 2\n",
    "dist_clusters = 1000\n",
    "splitting_dir = 'num_solve'\n",
    "num_pts_cka = 10000\n",
    "seed = 0\n",
    "experiment = '_one_class_one_point' # '' or '_one_class' or '_one_class_one_point'\n",
    "\n",
    "data_per_classes, indexes = get_activations_by_class(train_act, train_labels)\n",
    "lin_sep, cka, split_embeds, indexes, split_indexes = test_cka_lin_sep(data_per_classes, indexes, lin_svc, num_clusters = num_clusters, dist_clusters = dist_clusters, splitting_dir = splitting_dir, num_pts_cka = num_pts_cka, seed = seed, experiment = experiment)\n",
    "\n",
    "sorted_split_embeds = torch.Tensor(split_embeds[np.argsort(indexes)]).reshape([50000, 64, 2, 2])\n",
    "torch.save(sorted_split_embeds, 'data/cifar10_sorted_split_last_layer_embeds_{}num-clusters_{}dist-clusters_'.format(num_clusters, dist_clusters)+splitting_dir+'_{}pts-cka_{}seed'.format(num_pts_cka,seed)+experiment+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(sorted_split_embeds.numpy())\n",
    "# all the activations are positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025.486     53.81161]\n"
     ]
    }
   ],
   "source": [
    "norms = np.array([np.linalg.norm(sorted_split_embeds[i]) for i in range(50000)])\n",
    "print(norms[np.where(norms>50)])\n",
    "# There is only one point significantly far from the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "dist_clusters = 1000\n",
    "splitting_dir = 'num_solve'\n",
    "num_pts_cka = 10000\n",
    "seed = 0\n",
    "experiment = '' # '' or '_one_class_one_point' or '_one_class'\n",
    "\n",
    "#old_file = 'data/cifar10_sorted_split_last_layer_embeds_{}num-clusters_{}dist-clusters_'.format(num_clusters, dist_clusters)+splitting_dir+'_{}pts-cka_{}seed.pt'.format(num_pts_cka,seed)\n",
    "file_path = 'data/cifar10_sorted_split_last_layer_embeds_{}num-clusters_{}dist-clusters_'.format(num_clusters, dist_clusters)+splitting_dir+'_{}pts-cka_{}seed'.format(num_pts_cka,seed)+experiment+'.pt'\n",
    "\n",
    "class CIFAR10_mod(datasets.CIFAR10):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.split_embeds = torch.load(file_path)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        a = super().__getitem__(index)\n",
    "        return (a[0], a[1], index, self.split_embeds[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Import data \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize,])\n",
    "\n",
    "##### Loading CIFAR10 data\n",
    "cifar_data = CIFAR10_mod(root='.' ,train=True, transform=transform, download=True)\n",
    "cifar_data_test = datasets.CIFAR10(root='.' ,train=False, transform=transform, download=True)\n",
    "\n",
    "#### Creading data loaders\n",
    "train_loaderx = torch.utils.data.DataLoader(cifar_data, batch_size=2048, shuffle=False)\n",
    "test_loaderx = torch.utils.data.DataLoader(cifar_data_test, batch_size=10000, shuffle=False)\n",
    "\n",
    "# train_data, train_labels, train_indexes, train_embeds = iter(train_loaderx).next()\n",
    "# train_data = train_data.to(device)\n",
    "# train_labels = train_labels.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, data_loader, loss_fct):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label, index, split_embed) in enumerate(data_loader):\n",
    "            data, split_embed = data.to(device), split_embed.to(device)\n",
    "            output = model.forward_embed(data)\n",
    "            eval_loss += loss_fct(output, split_embed).detach()\n",
    "    return eval_loss\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "def train_embeds(model, device, train_loader, loss_fct = F.mse_loss, optimizer = torch.optim.Adam, epoch = 0, lr=0.1, layer_idx = -1, display=True):    \n",
    "    # Evaluating the loss on the whole dataset before starting training\n",
    "    before_training_loss = evaluate_loss(model, train_loader, loss_fct)\n",
    "    \n",
    "    # Training the model\n",
    "    model.train()\n",
    "    \n",
    "    ## Setting which layers to train and defining the optimizer\n",
    "    if layer_idx == -1:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"layers\" in name:\n",
    "                param.requires_grad = True\n",
    "#             if \"layers.6\" in name or \"layers.7\" in name:\n",
    "#                 param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "    optimizer = optimizer(model.parameters(), lr = lr)\n",
    "    \n",
    "    \n",
    "    for batch_idx, (data, label, index, split_embed) in enumerate(train_loader):\n",
    "        data, split_embed = data.to(device), split_embed.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward_embed(data)\n",
    "        loss = loss_fct(output, split_embed)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(batch_idx)\n",
    "#         print(len(data))\n",
    "    after_training_loss = evaluate_loss(model, train_loader, loss_fct)\n",
    "    \n",
    "    if display:\n",
    "#           print('Train Epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#           batch_idx * len(data), len(train_loader.dataset),\n",
    "#           100. * batch_idx / len(train_loader), loss.item()))\n",
    "        print('Epoch {} - Before training loss: {} | After training loss: {}'.format(epoch, before_training_loss, after_training_loss))\n",
    "    \n",
    "    return before_training_loss, after_training_loss\n",
    "    \n",
    "# Would need an option to train starting at a certain layer up to a certain layer (where the embedings were split)\n",
    "    \n",
    "def train_fc_layer(model, device, train_loader, loss_fct = F.cross_entropy, optimizer = torch.optim.Adam,  lr=0.1, display=True):\n",
    "    model.train()\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layers\" in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    optimizer = optimizer(model.parameters(), lr = lr)\n",
    "    for batch_idx, (data, label, index, split_embed) in enumerate(train_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fct(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if display:\n",
    "          print('Train Epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          batch_idx * len(data), len(train_loader.dataset),\n",
    "          100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "# Eventually would need a retrained for everything before and everything after a certain layer and not just fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_path)\n",
    "\n",
    "for loss_fct, loss_name in zip([F.mse_loss, nn.L1Loss()], ['MSE', 'L1']):\n",
    "# for loss_fct, loss_name in zip([nn.L1Loss()], ['L1']):\n",
    "    for optim, optim_name in zip([torch.optim.Adam, torch.optim.SGD],['Adam', 'SGD']):\n",
    "        for lr in [10, 1, 0.1, 0.01, 0.001]:\n",
    "            net = Net()\n",
    "            net.load_state_dict(torch.load('net_kornblith/net_kornblith_all_1.zip'))\n",
    "            net.cuda()\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))\n",
    "            for i in range(50):\n",
    "                a, b = train_embeds(net, device, train_loaderx, loss_fct = loss_fct, optimizer = optim, lr = lr, epoch = i, display=True)\n",
    "                if i == 0: initial_loss = a\n",
    "                if i == 24: final_loss = b\n",
    "            del net\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr)+' - Final / Initial loss %: {}'.format(final_loss/initial_loss*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training one for a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST: Loss: MSE | Optimizer: SGD | Lr: 0.1 - Final / Initial loss %: 66.11437225341797\n",
    "\n",
    "epochs = 800\n",
    "\n",
    "print(file_path)\n",
    "\n",
    "for loss_fct, loss_name in zip([F.mse_loss], ['MSE']):\n",
    "    for optim, optim_name in zip([torch.optim.SGD],['SGD']):\n",
    "        for lr in [0.1]:\n",
    "            net = Net()\n",
    "            net.load_state_dict(torch.load('net_kornblith/net_kornblith_all_1.zip'))\n",
    "            net.cuda()\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))\n",
    "            for i in range(epochs):\n",
    "                a, b = train_embeds(net, device, train_loaderx, loss_fct = loss_fct, optimizer = optim, lr = lr, epoch = i, display=True)\n",
    "#                 if i == 0: initial_loss = a\n",
    "#                 if i == 499: final_loss = b\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))#+' - Final / Initial loss %: {}'.format(final_loss/initial_loss*100))\n",
    "    \n",
    "torch.save(net.state_dict(), 'net_kornblith/net_kall1_split_train'+experiment+'_mse_sgd_lr0.1_{}e.t7'.format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training one for a long time with fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cifar10_sorted_split_last_layer_embeds_2num-clusters_1000dist-clusters_num_solve_10000pts-cka_0seed.pt\n",
      "Loss: MSE | Optimizer: SGD | Lr: 0.1\n",
      "Epoch 0 - Before training loss: 20358.17578125 | After training loss: 19262.54296875\n",
      "\n",
      "Test set: Average loss: 1.5841, Accuracy: 5130/10000 (51.30%)\n",
      "\n",
      "Epoch 1 - Before training loss: 19715.400390625 | After training loss: 19980.220703125\n",
      "Epoch 2 - Before training loss: 19980.220703125 | After training loss: 17843.4609375\n",
      "Epoch 3 - Before training loss: 17843.4609375 | After training loss: 18259.669921875\n",
      "Epoch 4 - Before training loss: 18259.669921875 | After training loss: 16863.1484375\n",
      "Epoch 5 - Before training loss: 16863.1484375 | After training loss: 16978.6796875\n",
      "\n",
      "Test set: Average loss: 1.5949, Accuracy: 4764/10000 (47.64%)\n",
      "\n",
      "Epoch 6 - Before training loss: 16650.60546875 | After training loss: 17068.44140625\n",
      "Epoch 7 - Before training loss: 17068.44140625 | After training loss: 15927.083984375\n",
      "Epoch 8 - Before training loss: 15927.083984375 | After training loss: 14687.8779296875\n",
      "Epoch 9 - Before training loss: 14687.8779296875 | After training loss: 16577.6328125\n",
      "Epoch 10 - Before training loss: 16577.6328125 | After training loss: 15758.876953125\n",
      "\n",
      "Test set: Average loss: 1.6566, Accuracy: 4859/10000 (48.59%)\n",
      "\n",
      "Epoch 11 - Before training loss: 14525.5869140625 | After training loss: 15423.2109375\n",
      "Epoch 12 - Before training loss: 15423.2109375 | After training loss: 15400.3876953125\n",
      "Epoch 13 - Before training loss: 15400.3876953125 | After training loss: 14452.087890625\n",
      "Epoch 14 - Before training loss: 14452.087890625 | After training loss: 15604.7158203125\n",
      "Epoch 15 - Before training loss: 15604.7158203125 | After training loss: 16379.873046875\n",
      "\n",
      "Test set: Average loss: 2.8819, Accuracy: 3799/10000 (37.99%)\n",
      "\n",
      "Epoch 16 - Before training loss: 13224.1220703125 | After training loss: 15078.203125\n",
      "Epoch 17 - Before training loss: 15078.203125 | After training loss: 15353.7568359375\n",
      "Epoch 18 - Before training loss: 15353.7568359375 | After training loss: 13589.736328125\n",
      "Epoch 19 - Before training loss: 13589.736328125 | After training loss: 12875.1015625\n",
      "Epoch 20 - Before training loss: 12875.1015625 | After training loss: 14033.44140625\n",
      "\n",
      "Test set: Average loss: 4.1456, Accuracy: 3353/10000 (33.53%)\n",
      "\n",
      "Epoch 21 - Before training loss: 11386.75390625 | After training loss: 13964.65234375\n",
      "Epoch 22 - Before training loss: 13964.65234375 | After training loss: 12906.7314453125\n",
      "Epoch 23 - Before training loss: 12906.7314453125 | After training loss: 13591.4208984375\n",
      "Epoch 24 - Before training loss: 13591.4208984375 | After training loss: 13041.3193359375\n",
      "Epoch 25 - Before training loss: 13041.3193359375 | After training loss: 13099.140625\n",
      "\n",
      "Test set: Average loss: 7.3961, Accuracy: 2102/10000 (21.02%)\n",
      "\n",
      "Epoch 26 - Before training loss: 10693.0029296875 | After training loss: 12273.1787109375\n",
      "Epoch 27 - Before training loss: 12273.1787109375 | After training loss: 13162.056640625\n",
      "Epoch 28 - Before training loss: 13162.056640625 | After training loss: 10686.8486328125\n",
      "Epoch 29 - Before training loss: 10686.8486328125 | After training loss: 12949.986328125\n",
      "Epoch 30 - Before training loss: 12949.986328125 | After training loss: 10548.1552734375\n",
      "\n",
      "Test set: Average loss: 7.9065, Accuracy: 2427/10000 (24.27%)\n",
      "\n",
      "Epoch 31 - Before training loss: 9167.8125 | After training loss: 10664.560546875\n",
      "Epoch 32 - Before training loss: 10664.560546875 | After training loss: 12583.2919921875\n",
      "Epoch 33 - Before training loss: 12583.2919921875 | After training loss: 11310.5380859375\n",
      "Epoch 34 - Before training loss: 11310.5380859375 | After training loss: 10960.8251953125\n",
      "Epoch 35 - Before training loss: 10960.8251953125 | After training loss: 9994.42578125\n",
      "\n",
      "Test set: Average loss: 9.6706, Accuracy: 3421/10000 (34.21%)\n",
      "\n",
      "Epoch 36 - Before training loss: 8315.21875 | After training loss: 10554.8837890625\n",
      "Epoch 37 - Before training loss: 10554.8837890625 | After training loss: 9779.4833984375\n",
      "Epoch 38 - Before training loss: 9779.4833984375 | After training loss: 10354.2744140625\n",
      "Epoch 39 - Before training loss: 10354.2744140625 | After training loss: 9951.26953125\n",
      "Epoch 40 - Before training loss: 9951.26953125 | After training loss: 9624.46484375\n",
      "\n",
      "Test set: Average loss: 13.6199, Accuracy: 2611/10000 (26.11%)\n",
      "\n",
      "Epoch 41 - Before training loss: 7997.88427734375 | After training loss: 8985.33203125\n",
      "Epoch 42 - Before training loss: 8985.33203125 | After training loss: 9523.943359375\n",
      "Epoch 43 - Before training loss: 9523.943359375 | After training loss: 7749.23486328125\n",
      "Epoch 44 - Before training loss: 7749.23486328125 | After training loss: 9045.51953125\n",
      "Epoch 45 - Before training loss: 9045.51953125 | After training loss: 8110.001953125\n",
      "\n",
      "Test set: Average loss: 7.6762, Accuracy: 2598/10000 (25.98%)\n",
      "\n",
      "Epoch 46 - Before training loss: 6982.1708984375 | After training loss: 9248.7236328125\n",
      "Epoch 47 - Before training loss: 9248.7236328125 | After training loss: 8517.1103515625\n",
      "Epoch 48 - Before training loss: 8517.1103515625 | After training loss: 8458.78515625\n",
      "Epoch 49 - Before training loss: 8458.78515625 | After training loss: 7559.17041015625\n",
      "Epoch 50 - Before training loss: 7559.17041015625 | After training loss: 7355.78125\n",
      "\n",
      "Test set: Average loss: 18.5298, Accuracy: 2349/10000 (23.49%)\n",
      "\n",
      "Epoch 51 - Before training loss: 6443.18994140625 | After training loss: 6840.59619140625\n",
      "Epoch 52 - Before training loss: 6840.59619140625 | After training loss: 7453.08740234375\n",
      "Epoch 53 - Before training loss: 7453.08740234375 | After training loss: 7641.93505859375\n",
      "Epoch 54 - Before training loss: 7641.93505859375 | After training loss: 6849.17333984375\n",
      "Epoch 55 - Before training loss: 6849.17333984375 | After training loss: 6801.74560546875\n",
      "\n",
      "Test set: Average loss: 11.7876, Accuracy: 2293/10000 (22.93%)\n",
      "\n",
      "Epoch 56 - Before training loss: 5899.19091796875 | After training loss: 7597.92529296875\n",
      "Epoch 57 - Before training loss: 7597.92529296875 | After training loss: 6479.93505859375\n",
      "Epoch 58 - Before training loss: 6479.93505859375 | After training loss: 6007.255859375\n",
      "Epoch 59 - Before training loss: 6007.255859375 | After training loss: 6721.03369140625\n",
      "Epoch 60 - Before training loss: 6721.03369140625 | After training loss: 5955.44189453125\n",
      "\n",
      "Test set: Average loss: 12.2789, Accuracy: 2378/10000 (23.78%)\n",
      "\n",
      "Epoch 61 - Before training loss: 5199.89990234375 | After training loss: 7028.94677734375\n",
      "Epoch 62 - Before training loss: 7028.94677734375 | After training loss: 5970.4443359375\n",
      "Epoch 63 - Before training loss: 5970.4443359375 | After training loss: 6358.57666015625\n",
      "Epoch 64 - Before training loss: 6358.57666015625 | After training loss: 5541.0537109375\n",
      "Epoch 65 - Before training loss: 5541.0537109375 | After training loss: 5802.51904296875\n",
      "\n",
      "Test set: Average loss: 9.6781, Accuracy: 2856/10000 (28.56%)\n",
      "\n",
      "Epoch 66 - Before training loss: 5119.35107421875 | After training loss: 5588.4794921875\n",
      "Epoch 67 - Before training loss: 5588.4794921875 | After training loss: 6052.630859375\n",
      "Epoch 68 - Before training loss: 6052.630859375 | After training loss: 6227.2021484375\n",
      "Epoch 69 - Before training loss: 6227.2021484375 | After training loss: 5939.357421875\n",
      "Epoch 70 - Before training loss: 5939.357421875 | After training loss: 5565.39404296875\n",
      "\n",
      "Test set: Average loss: 12.4064, Accuracy: 2486/10000 (24.86%)\n",
      "\n",
      "Epoch 71 - Before training loss: 4816.37158203125 | After training loss: 5551.763671875\n",
      "Epoch 72 - Before training loss: 5551.763671875 | After training loss: 5267.03662109375\n",
      "Epoch 73 - Before training loss: 5267.03662109375 | After training loss: 4763.0498046875\n",
      "Epoch 74 - Before training loss: 4763.0498046875 | After training loss: 4811.07421875\n",
      "Epoch 75 - Before training loss: 4811.07421875 | After training loss: 4931.912109375\n",
      "\n",
      "Test set: Average loss: 14.5893, Accuracy: 1846/10000 (18.46%)\n",
      "\n",
      "Epoch 76 - Before training loss: 4343.97265625 | After training loss: 6080.6142578125\n",
      "Epoch 77 - Before training loss: 6080.6142578125 | After training loss: 4287.51220703125\n",
      "Epoch 78 - Before training loss: 4287.51220703125 | After training loss: 4963.38037109375\n",
      "Epoch 79 - Before training loss: 4963.38037109375 | After training loss: 5135.1943359375\n",
      "Epoch 80 - Before training loss: 5135.1943359375 | After training loss: 4721.5849609375\n",
      "\n",
      "Test set: Average loss: 10.9064, Accuracy: 1852/10000 (18.52%)\n",
      "\n",
      "Epoch 81 - Before training loss: 4139.001953125 | After training loss: 4565.9580078125\n",
      "Epoch 82 - Before training loss: 4565.9580078125 | After training loss: 4056.736328125\n",
      "Epoch 83 - Before training loss: 4056.736328125 | After training loss: 4417.38134765625\n",
      "Epoch 84 - Before training loss: 4417.38134765625 | After training loss: 4047.80078125\n",
      "Epoch 85 - Before training loss: 4047.80078125 | After training loss: 5126.9072265625\n",
      "\n",
      "Test set: Average loss: 18.5684, Accuracy: 2633/10000 (26.33%)\n",
      "\n",
      "Epoch 86 - Before training loss: 3917.931396484375 | After training loss: 4253.72802734375\n",
      "Epoch 87 - Before training loss: 4253.72802734375 | After training loss: 3943.563232421875\n",
      "Epoch 88 - Before training loss: 3943.563232421875 | After training loss: 4622.47119140625\n",
      "Epoch 89 - Before training loss: 4622.47119140625 | After training loss: 4454.91455078125\n",
      "Epoch 90 - Before training loss: 4454.91455078125 | After training loss: 3861.781494140625\n",
      "\n",
      "Test set: Average loss: 10.8615, Accuracy: 2817/10000 (28.17%)\n",
      "\n",
      "Epoch 91 - Before training loss: 3488.9287109375 | After training loss: 4861.20556640625\n",
      "Epoch 92 - Before training loss: 4861.20556640625 | After training loss: 4186.2998046875\n",
      "Epoch 93 - Before training loss: 4186.2998046875 | After training loss: 4328.95458984375\n",
      "Epoch 94 - Before training loss: 4328.95458984375 | After training loss: 3892.739501953125\n",
      "Epoch 95 - Before training loss: 3892.739501953125 | After training loss: 4240.70361328125\n",
      "\n",
      "Test set: Average loss: 18.2000, Accuracy: 2418/10000 (24.18%)\n",
      "\n",
      "Epoch 96 - Before training loss: 3390.640625 | After training loss: 4109.05859375\n",
      "Epoch 97 - Before training loss: 4109.05859375 | After training loss: 3806.96484375\n",
      "Epoch 98 - Before training loss: 3806.96484375 | After training loss: 3729.046142578125\n",
      "Epoch 99 - Before training loss: 3729.046142578125 | After training loss: 4523.62255859375\n",
      "Epoch 100 - Before training loss: 4523.62255859375 | After training loss: 3877.43896484375\n",
      "\n",
      "Test set: Average loss: 9.7440, Accuracy: 3057/10000 (30.57%)\n",
      "\n",
      "Epoch 101 - Before training loss: 3178.65283203125 | After training loss: 3853.0\n",
      "Epoch 102 - Before training loss: 3853.0 | After training loss: 3647.67529296875\n",
      "Epoch 103 - Before training loss: 3647.67529296875 | After training loss: 3599.2548828125\n",
      "Epoch 104 - Before training loss: 3599.2548828125 | After training loss: 3539.29443359375\n",
      "Epoch 105 - Before training loss: 3539.29443359375 | After training loss: 3982.85498046875\n",
      "\n",
      "Test set: Average loss: 10.3336, Accuracy: 3046/10000 (30.46%)\n",
      "\n",
      "Epoch 106 - Before training loss: 3257.99072265625 | After training loss: 3861.4892578125\n",
      "Epoch 107 - Before training loss: 3861.4892578125 | After training loss: 3337.83349609375\n",
      "Epoch 108 - Before training loss: 3337.83349609375 | After training loss: 3562.05712890625\n",
      "Epoch 109 - Before training loss: 3562.05712890625 | After training loss: 3697.7646484375\n",
      "Epoch 110 - Before training loss: 3697.7646484375 | After training loss: 3436.948974609375\n",
      "\n",
      "Test set: Average loss: 11.0825, Accuracy: 2970/10000 (29.70%)\n",
      "\n",
      "Epoch 111 - Before training loss: 2868.03125 | After training loss: 3511.08935546875\n",
      "Epoch 112 - Before training loss: 3511.08935546875 | After training loss: 3645.3818359375\n",
      "Epoch 113 - Before training loss: 3645.3818359375 | After training loss: 3521.766357421875\n",
      "Epoch 114 - Before training loss: 3521.766357421875 | After training loss: 3164.201904296875\n",
      "Epoch 115 - Before training loss: 3164.201904296875 | After training loss: 4049.327880859375\n",
      "\n",
      "Test set: Average loss: 13.4317, Accuracy: 3003/10000 (30.03%)\n",
      "\n",
      "Epoch 116 - Before training loss: 2979.136474609375 | After training loss: 3204.092041015625\n",
      "Epoch 117 - Before training loss: 3204.092041015625 | After training loss: 3291.5419921875\n",
      "Epoch 118 - Before training loss: 3291.5419921875 | After training loss: 3534.591796875\n",
      "Epoch 119 - Before training loss: 3534.591796875 | After training loss: 3577.181884765625\n",
      "Epoch 120 - Before training loss: 3577.181884765625 | After training loss: 3449.443603515625\n",
      "\n",
      "Test set: Average loss: 16.7004, Accuracy: 2470/10000 (24.70%)\n",
      "\n",
      "Epoch 121 - Before training loss: 2618.5458984375 | After training loss: 3007.697509765625\n",
      "Epoch 122 - Before training loss: 3007.697509765625 | After training loss: 3124.46923828125\n",
      "Epoch 123 - Before training loss: 3124.46923828125 | After training loss: 3127.984375\n",
      "Epoch 124 - Before training loss: 3127.984375 | After training loss: 3438.69287109375\n",
      "Epoch 125 - Before training loss: 3438.69287109375 | After training loss: 3050.046875\n",
      "\n",
      "Test set: Average loss: 10.4567, Accuracy: 3124/10000 (31.24%)\n",
      "\n",
      "Epoch 126 - Before training loss: 2457.0126953125 | After training loss: 2857.560546875\n",
      "Epoch 127 - Before training loss: 2857.560546875 | After training loss: 2886.593017578125\n",
      "Epoch 128 - Before training loss: 2886.593017578125 | After training loss: 3054.37353515625\n",
      "Epoch 129 - Before training loss: 3054.37353515625 | After training loss: 3562.9130859375\n",
      "Epoch 130 - Before training loss: 3562.9130859375 | After training loss: 3396.77734375\n",
      "\n",
      "Test set: Average loss: 7.7820, Accuracy: 3381/10000 (33.81%)\n",
      "\n",
      "Epoch 131 - Before training loss: 2510.86865234375 | After training loss: 2751.8193359375\n",
      "Epoch 132 - Before training loss: 2751.8193359375 | After training loss: 2912.957275390625\n",
      "Epoch 133 - Before training loss: 2912.957275390625 | After training loss: 3050.492431640625\n",
      "Epoch 134 - Before training loss: 3050.492431640625 | After training loss: 2883.60107421875\n",
      "Epoch 135 - Before training loss: 2883.60107421875 | After training loss: 3050.095458984375\n",
      "\n",
      "Test set: Average loss: 17.1872, Accuracy: 2815/10000 (28.15%)\n",
      "\n",
      "Epoch 136 - Before training loss: 2383.37060546875 | After training loss: 2975.21044921875\n",
      "Epoch 137 - Before training loss: 2975.21044921875 | After training loss: 2872.15234375\n",
      "Epoch 138 - Before training loss: 2872.15234375 | After training loss: 3178.986328125\n",
      "Epoch 139 - Before training loss: 3178.986328125 | After training loss: 2813.681884765625\n",
      "Epoch 140 - Before training loss: 2813.681884765625 | After training loss: 2727.976806640625\n",
      "\n",
      "Test set: Average loss: 13.7699, Accuracy: 2915/10000 (29.15%)\n",
      "\n",
      "Epoch 141 - Before training loss: 2365.109375 | After training loss: 3510.667724609375\n",
      "Epoch 142 - Before training loss: 3510.667724609375 | After training loss: 2575.420654296875\n",
      "Epoch 143 - Before training loss: 2575.420654296875 | After training loss: 2601.2158203125\n",
      "Epoch 144 - Before training loss: 2601.2158203125 | After training loss: 2397.766845703125\n",
      "Epoch 145 - Before training loss: 2397.766845703125 | After training loss: 2499.713623046875\n",
      "\n",
      "Test set: Average loss: 9.9544, Accuracy: 3414/10000 (34.14%)\n",
      "\n",
      "Epoch 146 - Before training loss: 2181.05029296875 | After training loss: 2513.88232421875\n",
      "Epoch 147 - Before training loss: 2513.88232421875 | After training loss: 2309.622314453125\n",
      "Epoch 148 - Before training loss: 2309.622314453125 | After training loss: 2546.02734375\n",
      "Epoch 149 - Before training loss: 2546.02734375 | After training loss: 2614.1416015625\n",
      "Epoch 150 - Before training loss: 2614.1416015625 | After training loss: 2665.937744140625\n",
      "\n",
      "Test set: Average loss: 13.5435, Accuracy: 2903/10000 (29.03%)\n",
      "\n",
      "Epoch 151 - Before training loss: 2185.665771484375 | After training loss: 2846.366943359375\n",
      "Epoch 152 - Before training loss: 2846.366943359375 | After training loss: 2540.765869140625\n",
      "Epoch 153 - Before training loss: 2540.765869140625 | After training loss: 2312.342041015625\n",
      "Epoch 154 - Before training loss: 2312.342041015625 | After training loss: 2906.7216796875\n",
      "Epoch 155 - Before training loss: 2906.7216796875 | After training loss: 2396.94677734375\n",
      "\n",
      "Test set: Average loss: 7.8035, Accuracy: 3397/10000 (33.97%)\n",
      "\n",
      "Epoch 156 - Before training loss: 2049.42431640625 | After training loss: 2307.078369140625\n",
      "Epoch 157 - Before training loss: 2307.078369140625 | After training loss: 2743.339111328125\n",
      "Epoch 158 - Before training loss: 2743.339111328125 | After training loss: 2247.117919921875\n",
      "Epoch 159 - Before training loss: 2247.117919921875 | After training loss: 2400.408447265625\n",
      "Epoch 160 - Before training loss: 2400.408447265625 | After training loss: 2471.054931640625\n",
      "\n",
      "Test set: Average loss: 12.0381, Accuracy: 2946/10000 (29.46%)\n",
      "\n",
      "Epoch 161 - Before training loss: 2031.7769775390625 | After training loss: 2559.223876953125\n",
      "Epoch 162 - Before training loss: 2559.223876953125 | After training loss: 2253.463623046875\n",
      "Epoch 163 - Before training loss: 2253.463623046875 | After training loss: 2304.261962890625\n",
      "Epoch 164 - Before training loss: 2304.261962890625 | After training loss: 2448.647216796875\n",
      "Epoch 165 - Before training loss: 2448.647216796875 | After training loss: 2647.6416015625\n",
      "\n",
      "Test set: Average loss: 13.6677, Accuracy: 2337/10000 (23.37%)\n",
      "\n",
      "Epoch 166 - Before training loss: 2027.75 | After training loss: 1934.31787109375\n",
      "Epoch 167 - Before training loss: 1934.31787109375 | After training loss: 2187.03076171875\n",
      "Epoch 168 - Before training loss: 2187.03076171875 | After training loss: 2220.345703125\n",
      "Epoch 169 - Before training loss: 2220.345703125 | After training loss: 2260.849609375\n",
      "Epoch 170 - Before training loss: 2260.849609375 | After training loss: 2307.84765625\n",
      "\n",
      "Test set: Average loss: 8.4263, Accuracy: 3145/10000 (31.45%)\n",
      "\n",
      "Epoch 171 - Before training loss: 1841.1630859375 | After training loss: 1918.1708984375\n",
      "Epoch 172 - Before training loss: 1918.1708984375 | After training loss: 2079.654541015625\n",
      "Epoch 173 - Before training loss: 2079.654541015625 | After training loss: 2558.615478515625\n",
      "Epoch 174 - Before training loss: 2558.615478515625 | After training loss: 2338.174072265625\n",
      "Epoch 175 - Before training loss: 2338.174072265625 | After training loss: 2324.957275390625\n",
      "\n",
      "Test set: Average loss: 8.8312, Accuracy: 3280/10000 (32.80%)\n",
      "\n",
      "Epoch 176 - Before training loss: 1827.536865234375 | After training loss: 2185.563232421875\n",
      "Epoch 177 - Before training loss: 2185.563232421875 | After training loss: 2599.364013671875\n",
      "Epoch 178 - Before training loss: 2599.364013671875 | After training loss: 2279.961181640625\n",
      "Epoch 179 - Before training loss: 2279.961181640625 | After training loss: 1952.9266357421875\n",
      "Epoch 180 - Before training loss: 1952.9266357421875 | After training loss: 1994.218017578125\n",
      "\n",
      "Test set: Average loss: 9.0546, Accuracy: 2922/10000 (29.22%)\n",
      "\n",
      "Epoch 181 - Before training loss: 1721.2008056640625 | After training loss: 2310.1611328125\n",
      "Epoch 182 - Before training loss: 2310.1611328125 | After training loss: 1816.1064453125\n",
      "Epoch 183 - Before training loss: 1816.1064453125 | After training loss: 1953.3330078125\n",
      "Epoch 184 - Before training loss: 1953.3330078125 | After training loss: 2067.416748046875\n",
      "Epoch 185 - Before training loss: 2067.416748046875 | After training loss: 1966.5618896484375\n",
      "\n",
      "Test set: Average loss: 10.3933, Accuracy: 2975/10000 (29.75%)\n",
      "\n",
      "Epoch 186 - Before training loss: 1704.871337890625 | After training loss: 2015.0543212890625\n",
      "Epoch 187 - Before training loss: 2015.0543212890625 | After training loss: 2023.2266845703125\n",
      "Epoch 188 - Before training loss: 2023.2266845703125 | After training loss: 1802.584716796875\n",
      "Epoch 189 - Before training loss: 1802.584716796875 | After training loss: 2088.148681640625\n",
      "Epoch 190 - Before training loss: 2088.148681640625 | After training loss: 1935.869140625\n",
      "\n",
      "Test set: Average loss: 14.2772, Accuracy: 2998/10000 (29.98%)\n",
      "\n",
      "Epoch 191 - Before training loss: 1740.4398193359375 | After training loss: 1892.6580810546875\n",
      "Epoch 192 - Before training loss: 1892.6580810546875 | After training loss: 2008.960693359375\n",
      "Epoch 193 - Before training loss: 2008.960693359375 | After training loss: 1715.820556640625\n",
      "Epoch 194 - Before training loss: 1715.820556640625 | After training loss: 1752.4869384765625\n",
      "Epoch 195 - Before training loss: 1752.4869384765625 | After training loss: 2217.8515625\n",
      "\n",
      "Test set: Average loss: 7.9098, Accuracy: 3251/10000 (32.51%)\n",
      "\n",
      "Epoch 196 - Before training loss: 1741.78955078125 | After training loss: 1797.8280029296875\n",
      "Epoch 197 - Before training loss: 1797.8280029296875 | After training loss: 1845.03369140625\n",
      "Epoch 198 - Before training loss: 1845.03369140625 | After training loss: 1919.235107421875\n",
      "Epoch 199 - Before training loss: 1919.235107421875 | After training loss: 1825.5830078125\n",
      "Epoch 200 - Before training loss: 1825.5830078125 | After training loss: 1842.1005859375\n",
      "\n",
      "Test set: Average loss: 9.1281, Accuracy: 2790/10000 (27.90%)\n",
      "\n",
      "Epoch 201 - Before training loss: 1535.4556884765625 | After training loss: 1921.361083984375\n",
      "Epoch 202 - Before training loss: 1921.361083984375 | After training loss: 1622.1912841796875\n",
      "Epoch 203 - Before training loss: 1622.1912841796875 | After training loss: 2013.8115234375\n",
      "Epoch 204 - Before training loss: 2013.8115234375 | After training loss: 1778.218505859375\n",
      "Epoch 205 - Before training loss: 1778.218505859375 | After training loss: 1807.760986328125\n",
      "\n",
      "Test set: Average loss: 15.3481, Accuracy: 2423/10000 (24.23%)\n",
      "\n",
      "Epoch 206 - Before training loss: 1428.92529296875 | After training loss: 1660.8890380859375\n",
      "Epoch 207 - Before training loss: 1660.8890380859375 | After training loss: 1776.9798583984375\n",
      "Epoch 208 - Before training loss: 1776.9798583984375 | After training loss: 1893.7198486328125\n",
      "Epoch 209 - Before training loss: 1893.7198486328125 | After training loss: 1710.7520751953125\n",
      "Epoch 210 - Before training loss: 1710.7520751953125 | After training loss: 1498.2943115234375\n",
      "\n",
      "Test set: Average loss: 9.3864, Accuracy: 2929/10000 (29.29%)\n",
      "\n",
      "Epoch 211 - Before training loss: 1375.4437255859375 | After training loss: 1657.001220703125\n",
      "Epoch 212 - Before training loss: 1657.001220703125 | After training loss: 1822.4715576171875\n",
      "Epoch 213 - Before training loss: 1822.4715576171875 | After training loss: 1928.21142578125\n",
      "Epoch 214 - Before training loss: 1928.21142578125 | After training loss: 1697.4671630859375\n",
      "Epoch 215 - Before training loss: 1697.4671630859375 | After training loss: 1704.716796875\n",
      "\n",
      "Test set: Average loss: 9.2689, Accuracy: 3263/10000 (32.63%)\n",
      "\n",
      "Epoch 216 - Before training loss: 1399.6785888671875 | After training loss: 1620.4600830078125\n",
      "Epoch 217 - Before training loss: 1620.4600830078125 | After training loss: 1695.9853515625\n",
      "Epoch 218 - Before training loss: 1695.9853515625 | After training loss: 1635.465087890625\n",
      "Epoch 219 - Before training loss: 1635.465087890625 | After training loss: 1537.27783203125\n",
      "Epoch 220 - Before training loss: 1537.27783203125 | After training loss: 1521.4644775390625\n",
      "\n",
      "Test set: Average loss: 9.3030, Accuracy: 3001/10000 (30.01%)\n",
      "\n",
      "Epoch 221 - Before training loss: 1339.2911376953125 | After training loss: 1988.988037109375\n",
      "Epoch 222 - Before training loss: 1988.988037109375 | After training loss: 1776.7548828125\n",
      "Epoch 223 - Before training loss: 1776.7548828125 | After training loss: 1842.86328125\n",
      "Epoch 224 - Before training loss: 1842.86328125 | After training loss: 1873.7877197265625\n",
      "Epoch 225 - Before training loss: 1873.7877197265625 | After training loss: 1729.027587890625\n",
      "\n",
      "Test set: Average loss: 13.8618, Accuracy: 2989/10000 (29.89%)\n",
      "\n",
      "Epoch 226 - Before training loss: 1414.2177734375 | After training loss: 1793.047607421875\n",
      "Epoch 227 - Before training loss: 1793.047607421875 | After training loss: 1672.7406005859375\n",
      "Epoch 228 - Before training loss: 1672.7406005859375 | After training loss: 1528.220703125\n",
      "Epoch 229 - Before training loss: 1528.220703125 | After training loss: 1618.357421875\n",
      "Epoch 230 - Before training loss: 1618.357421875 | After training loss: 1592.242431640625\n",
      "\n",
      "Test set: Average loss: 8.2929, Accuracy: 2940/10000 (29.40%)\n",
      "\n",
      "Epoch 231 - Before training loss: 1345.7989501953125 | After training loss: 1479.7161865234375\n",
      "Epoch 232 - Before training loss: 1479.7161865234375 | After training loss: 1568.9488525390625\n",
      "Epoch 233 - Before training loss: 1568.9488525390625 | After training loss: 1440.96875\n",
      "Epoch 234 - Before training loss: 1440.96875 | After training loss: 1723.6568603515625\n",
      "Epoch 235 - Before training loss: 1723.6568603515625 | After training loss: 1585.02587890625\n",
      "\n",
      "Test set: Average loss: 10.0690, Accuracy: 3235/10000 (32.35%)\n",
      "\n",
      "Epoch 236 - Before training loss: 1302.4378662109375 | After training loss: 1436.71484375\n",
      "Epoch 237 - Before training loss: 1436.71484375 | After training loss: 1496.8914794921875\n",
      "Epoch 238 - Before training loss: 1496.8914794921875 | After training loss: 1550.1427001953125\n",
      "Epoch 239 - Before training loss: 1550.1427001953125 | After training loss: 1596.386474609375\n",
      "Epoch 240 - Before training loss: 1596.386474609375 | After training loss: 1771.11865234375\n",
      "\n",
      "Test set: Average loss: 11.9730, Accuracy: 3100/10000 (31.00%)\n",
      "\n",
      "Epoch 241 - Before training loss: 1452.008056640625 | After training loss: 1374.8775634765625\n",
      "Epoch 242 - Before training loss: 1374.8775634765625 | After training loss: 1469.275146484375\n",
      "Epoch 243 - Before training loss: 1469.275146484375 | After training loss: 1465.20654296875\n",
      "Epoch 244 - Before training loss: 1465.20654296875 | After training loss: 1368.69189453125\n",
      "Epoch 245 - Before training loss: 1368.69189453125 | After training loss: 1455.2535400390625\n",
      "\n",
      "Test set: Average loss: 8.8759, Accuracy: 2902/10000 (29.02%)\n",
      "\n",
      "Epoch 246 - Before training loss: 1193.433349609375 | After training loss: 1450.59716796875\n",
      "Epoch 247 - Before training loss: 1450.59716796875 | After training loss: 1827.2239990234375\n",
      "Epoch 248 - Before training loss: 1827.2239990234375 | After training loss: 1515.392578125\n",
      "Epoch 249 - Before training loss: 1515.392578125 | After training loss: 1437.239990234375\n",
      "Epoch 250 - Before training loss: 1437.239990234375 | After training loss: 1735.08154296875\n",
      "\n",
      "Test set: Average loss: 7.4723, Accuracy: 3561/10000 (35.61%)\n",
      "\n",
      "Epoch 251 - Before training loss: 1288.13720703125 | After training loss: 1663.94140625\n",
      "Epoch 252 - Before training loss: 1663.94140625 | After training loss: 1662.0631103515625\n",
      "Epoch 253 - Before training loss: 1662.0631103515625 | After training loss: 1524.8314208984375\n",
      "Epoch 254 - Before training loss: 1524.8314208984375 | After training loss: 1447.9703369140625\n",
      "Epoch 255 - Before training loss: 1447.9703369140625 | After training loss: 1447.167236328125\n",
      "\n",
      "Test set: Average loss: 9.2443, Accuracy: 2871/10000 (28.71%)\n",
      "\n",
      "Epoch 256 - Before training loss: 1239.3154296875 | After training loss: 1587.119384765625\n",
      "Epoch 257 - Before training loss: 1587.119384765625 | After training loss: 1381.4334716796875\n",
      "Epoch 258 - Before training loss: 1381.4334716796875 | After training loss: 1509.5911865234375\n",
      "Epoch 259 - Before training loss: 1509.5911865234375 | After training loss: 1374.1168212890625\n",
      "Epoch 260 - Before training loss: 1374.1168212890625 | After training loss: 1504.0953369140625\n",
      "\n",
      "Test set: Average loss: 12.0438, Accuracy: 2771/10000 (27.71%)\n",
      "\n",
      "Epoch 261 - Before training loss: 1153.9278564453125 | After training loss: 1313.3192138671875\n",
      "Epoch 262 - Before training loss: 1313.3192138671875 | After training loss: 1496.0413818359375\n",
      "Epoch 263 - Before training loss: 1496.0413818359375 | After training loss: 1485.9359130859375\n",
      "Epoch 264 - Before training loss: 1485.9359130859375 | After training loss: 1270.84375\n",
      "Epoch 265 - Before training loss: 1270.84375 | After training loss: 1269.59765625\n",
      "\n",
      "Test set: Average loss: 6.4369, Accuracy: 3365/10000 (33.65%)\n",
      "\n",
      "Epoch 266 - Before training loss: 1087.0443115234375 | After training loss: 1232.0440673828125\n",
      "Epoch 267 - Before training loss: 1232.0440673828125 | After training loss: 1330.6021728515625\n",
      "Epoch 268 - Before training loss: 1330.6021728515625 | After training loss: 1293.0462646484375\n",
      "Epoch 269 - Before training loss: 1293.0462646484375 | After training loss: 1435.243896484375\n",
      "Epoch 270 - Before training loss: 1435.243896484375 | After training loss: 1509.5953369140625\n",
      "\n",
      "Test set: Average loss: 9.8596, Accuracy: 2813/10000 (28.13%)\n",
      "\n",
      "Epoch 271 - Before training loss: 1268.0970458984375 | After training loss: 1588.020751953125\n",
      "Epoch 272 - Before training loss: 1588.020751953125 | After training loss: 1369.2672119140625\n",
      "Epoch 273 - Before training loss: 1369.2672119140625 | After training loss: 1305.3861083984375\n",
      "Epoch 274 - Before training loss: 1305.3861083984375 | After training loss: 1244.4168701171875\n",
      "Epoch 275 - Before training loss: 1244.4168701171875 | After training loss: 1415.1663818359375\n",
      "\n",
      "Test set: Average loss: 7.5962, Accuracy: 3262/10000 (32.62%)\n",
      "\n",
      "Epoch 276 - Before training loss: 1113.3812255859375 | After training loss: 1179.140625\n",
      "Epoch 277 - Before training loss: 1179.140625 | After training loss: 1241.5421142578125\n",
      "Epoch 278 - Before training loss: 1241.5421142578125 | After training loss: 1224.26806640625\n",
      "Epoch 279 - Before training loss: 1224.26806640625 | After training loss: 1558.6275634765625\n",
      "Epoch 280 - Before training loss: 1558.6275634765625 | After training loss: 1308.9537353515625\n",
      "\n",
      "Test set: Average loss: 7.1997, Accuracy: 2999/10000 (29.99%)\n",
      "\n",
      "Epoch 281 - Before training loss: 1144.655029296875 | After training loss: 1207.6358642578125\n",
      "Epoch 282 - Before training loss: 1207.6358642578125 | After training loss: 1117.482666015625\n",
      "Epoch 283 - Before training loss: 1117.482666015625 | After training loss: 1383.5966796875\n",
      "Epoch 284 - Before training loss: 1383.5966796875 | After training loss: 1228.5101318359375\n",
      "Epoch 285 - Before training loss: 1228.5101318359375 | After training loss: 1228.57421875\n",
      "\n",
      "Test set: Average loss: 10.8516, Accuracy: 2753/10000 (27.53%)\n",
      "\n",
      "Epoch 286 - Before training loss: 1048.6202392578125 | After training loss: 1202.525634765625\n",
      "Epoch 287 - Before training loss: 1202.525634765625 | After training loss: 1223.5777587890625\n",
      "Epoch 288 - Before training loss: 1223.5777587890625 | After training loss: 1306.3175048828125\n",
      "Epoch 289 - Before training loss: 1306.3175048828125 | After training loss: 1461.245849609375\n",
      "Epoch 290 - Before training loss: 1461.245849609375 | After training loss: 1240.6512451171875\n",
      "\n",
      "Test set: Average loss: 5.9794, Accuracy: 3066/10000 (30.66%)\n",
      "\n",
      "Epoch 291 - Before training loss: 1079.15234375 | After training loss: 1375.2928466796875\n",
      "Epoch 292 - Before training loss: 1375.2928466796875 | After training loss: 1193.2236328125\n",
      "Epoch 293 - Before training loss: 1193.2236328125 | After training loss: 1195.1224365234375\n",
      "Epoch 294 - Before training loss: 1195.1224365234375 | After training loss: 1114.1192626953125\n",
      "Epoch 295 - Before training loss: 1114.1192626953125 | After training loss: 1168.125732421875\n",
      "\n",
      "Test set: Average loss: 5.4409, Accuracy: 3137/10000 (31.37%)\n",
      "\n",
      "Epoch 296 - Before training loss: 1005.2938232421875 | After training loss: 1128.7823486328125\n",
      "Epoch 297 - Before training loss: 1128.7823486328125 | After training loss: 1130.864013671875\n",
      "Epoch 298 - Before training loss: 1130.864013671875 | After training loss: 1110.481201171875\n",
      "Epoch 299 - Before training loss: 1110.481201171875 | After training loss: 1116.61083984375\n",
      "Epoch 300 - Before training loss: 1116.61083984375 | After training loss: 1074.5689697265625\n",
      "\n",
      "Test set: Average loss: 8.4335, Accuracy: 2805/10000 (28.05%)\n",
      "\n",
      "Epoch 301 - Before training loss: 953.3836059570312 | After training loss: 1179.0999755859375\n",
      "Epoch 302 - Before training loss: 1179.0999755859375 | After training loss: 1288.710693359375\n",
      "Epoch 303 - Before training loss: 1288.710693359375 | After training loss: 1322.3023681640625\n",
      "Epoch 304 - Before training loss: 1322.3023681640625 | After training loss: 1159.71337890625\n",
      "Epoch 305 - Before training loss: 1159.71337890625 | After training loss: 1173.42919921875\n",
      "\n",
      "Test set: Average loss: 9.9092, Accuracy: 2973/10000 (29.73%)\n",
      "\n",
      "Epoch 306 - Before training loss: 980.9117431640625 | After training loss: 1176.2286376953125\n",
      "Epoch 307 - Before training loss: 1176.2286376953125 | After training loss: 1247.78125\n",
      "Epoch 308 - Before training loss: 1247.78125 | After training loss: 1017.0512084960938\n",
      "Epoch 309 - Before training loss: 1017.0512084960938 | After training loss: 1069.911865234375\n",
      "Epoch 310 - Before training loss: 1069.911865234375 | After training loss: 1171.9913330078125\n",
      "\n",
      "Test set: Average loss: 9.1568, Accuracy: 2929/10000 (29.29%)\n",
      "\n",
      "Epoch 311 - Before training loss: 989.5593872070312 | After training loss: 1296.679931640625\n",
      "Epoch 312 - Before training loss: 1296.679931640625 | After training loss: 1139.5096435546875\n",
      "Epoch 313 - Before training loss: 1139.5096435546875 | After training loss: 1072.1822509765625\n",
      "Epoch 314 - Before training loss: 1072.1822509765625 | After training loss: 1136.8214111328125\n",
      "Epoch 315 - Before training loss: 1136.8214111328125 | After training loss: 1244.268798828125\n",
      "\n",
      "Test set: Average loss: 12.1888, Accuracy: 2635/10000 (26.35%)\n",
      "\n",
      "Epoch 316 - Before training loss: 1073.5723876953125 | After training loss: 1232.31884765625\n",
      "Epoch 317 - Before training loss: 1232.31884765625 | After training loss: 1075.6806640625\n",
      "Epoch 318 - Before training loss: 1075.6806640625 | After training loss: 1000.77587890625\n",
      "Epoch 319 - Before training loss: 1000.77587890625 | After training loss: 1118.6920166015625\n",
      "Epoch 320 - Before training loss: 1118.6920166015625 | After training loss: 1220.430908203125\n",
      "\n",
      "Test set: Average loss: 8.3180, Accuracy: 3067/10000 (30.67%)\n",
      "\n",
      "Epoch 321 - Before training loss: 993.2024536132812 | After training loss: 1061.4930419921875\n",
      "Epoch 322 - Before training loss: 1061.4930419921875 | After training loss: 994.3828125\n",
      "Epoch 323 - Before training loss: 994.3828125 | After training loss: 949.2874755859375\n",
      "Epoch 324 - Before training loss: 949.2874755859375 | After training loss: 1040.20751953125\n",
      "Epoch 325 - Before training loss: 1040.20751953125 | After training loss: 1021.0679321289062\n",
      "\n",
      "Test set: Average loss: 10.2038, Accuracy: 2627/10000 (26.27%)\n",
      "\n",
      "Epoch 326 - Before training loss: 895.785400390625 | After training loss: 936.2130126953125\n",
      "Epoch 327 - Before training loss: 936.2130126953125 | After training loss: 896.5399169921875\n",
      "Epoch 328 - Before training loss: 896.5399169921875 | After training loss: 1077.8448486328125\n",
      "Epoch 329 - Before training loss: 1077.8448486328125 | After training loss: 1015.2452392578125\n",
      "Epoch 330 - Before training loss: 1015.2452392578125 | After training loss: 1044.952392578125\n",
      "\n",
      "Test set: Average loss: 8.3040, Accuracy: 2721/10000 (27.21%)\n",
      "\n",
      "Epoch 331 - Before training loss: 911.6234741210938 | After training loss: 930.9554443359375\n",
      "Epoch 332 - Before training loss: 930.9554443359375 | After training loss: 1128.19384765625\n",
      "Epoch 333 - Before training loss: 1128.19384765625 | After training loss: 1143.875244140625\n",
      "Epoch 334 - Before training loss: 1143.875244140625 | After training loss: 1125.7691650390625\n",
      "Epoch 335 - Before training loss: 1125.7691650390625 | After training loss: 1093.1483154296875\n",
      "\n",
      "Test set: Average loss: 7.5762, Accuracy: 2926/10000 (29.26%)\n",
      "\n",
      "Epoch 336 - Before training loss: 920.5409545898438 | After training loss: 1164.348388671875\n",
      "Epoch 337 - Before training loss: 1164.348388671875 | After training loss: 1113.1302490234375\n",
      "Epoch 338 - Before training loss: 1113.1302490234375 | After training loss: 1001.6316528320312\n",
      "Epoch 339 - Before training loss: 1001.6316528320312 | After training loss: 952.7847290039062\n",
      "Epoch 340 - Before training loss: 952.7847290039062 | After training loss: 1100.57958984375\n",
      "\n",
      "Test set: Average loss: 10.9824, Accuracy: 2615/10000 (26.15%)\n",
      "\n",
      "Epoch 341 - Before training loss: 896.3303833007812 | After training loss: 1017.9828491210938\n",
      "Epoch 342 - Before training loss: 1017.9828491210938 | After training loss: 1024.0770263671875\n",
      "Epoch 343 - Before training loss: 1024.0770263671875 | After training loss: 1075.196044921875\n",
      "Epoch 344 - Before training loss: 1075.196044921875 | After training loss: 964.4928588867188\n",
      "Epoch 345 - Before training loss: 964.4928588867188 | After training loss: 972.1544189453125\n",
      "\n",
      "Test set: Average loss: 4.6652, Accuracy: 3215/10000 (32.15%)\n",
      "\n",
      "Epoch 346 - Before training loss: 830.6204223632812 | After training loss: 1081.5545654296875\n",
      "Epoch 347 - Before training loss: 1081.5545654296875 | After training loss: 1000.0933227539062\n",
      "Epoch 348 - Before training loss: 1000.0933227539062 | After training loss: 983.7786254882812\n",
      "Epoch 349 - Before training loss: 983.7786254882812 | After training loss: 825.01416015625\n",
      "Epoch 350 - Before training loss: 825.01416015625 | After training loss: 1020.560302734375\n",
      "\n",
      "Test set: Average loss: 9.3496, Accuracy: 2967/10000 (29.67%)\n",
      "\n",
      "Epoch 351 - Before training loss: 832.0701293945312 | After training loss: 889.157470703125\n",
      "Epoch 352 - Before training loss: 889.157470703125 | After training loss: 920.258544921875\n",
      "Epoch 353 - Before training loss: 920.258544921875 | After training loss: 947.3470458984375\n",
      "Epoch 354 - Before training loss: 947.3470458984375 | After training loss: 872.2113647460938\n",
      "Epoch 355 - Before training loss: 872.2113647460938 | After training loss: 1019.1539306640625\n",
      "\n",
      "Test set: Average loss: 9.6652, Accuracy: 2826/10000 (28.26%)\n",
      "\n",
      "Epoch 356 - Before training loss: 867.0628051757812 | After training loss: 1135.753662109375\n",
      "Epoch 357 - Before training loss: 1135.753662109375 | After training loss: 1148.1297607421875\n",
      "Epoch 358 - Before training loss: 1148.1297607421875 | After training loss: 1201.66552734375\n",
      "Epoch 359 - Before training loss: 1201.66552734375 | After training loss: 967.59912109375\n",
      "Epoch 360 - Before training loss: 967.59912109375 | After training loss: 884.3472900390625\n",
      "\n",
      "Test set: Average loss: 9.6657, Accuracy: 2820/10000 (28.20%)\n",
      "\n",
      "Epoch 361 - Before training loss: 797.3974609375 | After training loss: 928.5513916015625\n",
      "Epoch 362 - Before training loss: 928.5513916015625 | After training loss: 849.8536376953125\n",
      "Epoch 363 - Before training loss: 849.8536376953125 | After training loss: 793.6522216796875\n",
      "Epoch 364 - Before training loss: 793.6522216796875 | After training loss: 915.3955688476562\n",
      "Epoch 365 - Before training loss: 915.3955688476562 | After training loss: 948.3101196289062\n",
      "\n",
      "Test set: Average loss: 6.6074, Accuracy: 3048/10000 (30.48%)\n",
      "\n",
      "Epoch 366 - Before training loss: 880.8226928710938 | After training loss: 1199.0697021484375\n",
      "Epoch 367 - Before training loss: 1199.0697021484375 | After training loss: 1013.9107666015625\n",
      "Epoch 368 - Before training loss: 1013.9107666015625 | After training loss: 890.7269287109375\n",
      "Epoch 369 - Before training loss: 890.7269287109375 | After training loss: 835.0050048828125\n",
      "Epoch 370 - Before training loss: 835.0050048828125 | After training loss: 853.6300659179688\n",
      "\n",
      "Test set: Average loss: 10.8902, Accuracy: 3006/10000 (30.06%)\n",
      "\n",
      "Epoch 371 - Before training loss: 756.5784912109375 | After training loss: 856.78076171875\n",
      "Epoch 372 - Before training loss: 856.78076171875 | After training loss: 862.8551635742188\n",
      "Epoch 373 - Before training loss: 862.8551635742188 | After training loss: 771.53125\n",
      "Epoch 374 - Before training loss: 771.53125 | After training loss: 817.8917846679688\n",
      "Epoch 375 - Before training loss: 817.8917846679688 | After training loss: 856.4185791015625\n",
      "\n",
      "Test set: Average loss: 6.7242, Accuracy: 3063/10000 (30.63%)\n",
      "\n",
      "Epoch 376 - Before training loss: 733.949951171875 | After training loss: 862.6561279296875\n",
      "Epoch 377 - Before training loss: 862.6561279296875 | After training loss: 880.6691284179688\n",
      "Epoch 378 - Before training loss: 880.6691284179688 | After training loss: 920.6000366210938\n",
      "Epoch 379 - Before training loss: 920.6000366210938 | After training loss: 929.8685302734375\n",
      "Epoch 380 - Before training loss: 929.8685302734375 | After training loss: 961.198974609375\n",
      "\n",
      "Test set: Average loss: 6.5957, Accuracy: 3247/10000 (32.47%)\n",
      "\n",
      "Epoch 381 - Before training loss: 805.520751953125 | After training loss: 833.7959594726562\n",
      "Epoch 382 - Before training loss: 833.7959594726562 | After training loss: 994.1892700195312\n",
      "Epoch 383 - Before training loss: 994.1892700195312 | After training loss: 940.5971069335938\n",
      "Epoch 384 - Before training loss: 940.5971069335938 | After training loss: 866.0445556640625\n",
      "Epoch 385 - Before training loss: 866.0445556640625 | After training loss: 791.7334594726562\n",
      "\n",
      "Test set: Average loss: 6.7429, Accuracy: 3010/10000 (30.10%)\n",
      "\n",
      "Epoch 386 - Before training loss: 692.4404296875 | After training loss: 765.5221557617188\n",
      "Epoch 387 - Before training loss: 765.5221557617188 | After training loss: 749.1914672851562\n",
      "Epoch 388 - Before training loss: 749.1914672851562 | After training loss: 778.2093505859375\n",
      "Epoch 389 - Before training loss: 778.2093505859375 | After training loss: 830.7280883789062\n",
      "Epoch 390 - Before training loss: 830.7280883789062 | After training loss: 1050.1217041015625\n",
      "\n",
      "Test set: Average loss: 7.9284, Accuracy: 3032/10000 (30.32%)\n",
      "\n",
      "Epoch 391 - Before training loss: 823.7689819335938 | After training loss: 809.6505737304688\n",
      "Epoch 392 - Before training loss: 809.6505737304688 | After training loss: 733.3578491210938\n",
      "Epoch 393 - Before training loss: 733.3578491210938 | After training loss: 693.8142700195312\n",
      "Epoch 394 - Before training loss: 693.8142700195312 | After training loss: 809.225830078125\n",
      "Epoch 395 - Before training loss: 809.225830078125 | After training loss: 794.5393676757812\n",
      "\n",
      "Test set: Average loss: 8.0464, Accuracy: 2946/10000 (29.46%)\n",
      "\n",
      "Epoch 396 - Before training loss: 713.060546875 | After training loss: 855.83984375\n",
      "Epoch 397 - Before training loss: 855.83984375 | After training loss: 785.7093505859375\n",
      "Epoch 398 - Before training loss: 785.7093505859375 | After training loss: 878.8378295898438\n",
      "Epoch 399 - Before training loss: 878.8378295898438 | After training loss: 881.8936157226562\n",
      "Epoch 400 - Before training loss: 881.8936157226562 | After training loss: 940.15576171875\n",
      "\n",
      "Test set: Average loss: 8.4152, Accuracy: 2952/10000 (29.52%)\n",
      "\n",
      "Epoch 401 - Before training loss: 806.4267578125 | After training loss: 798.3247680664062\n",
      "Epoch 402 - Before training loss: 798.3247680664062 | After training loss: 755.8287963867188\n",
      "Epoch 403 - Before training loss: 755.8287963867188 | After training loss: 733.1942138671875\n",
      "Epoch 404 - Before training loss: 733.1942138671875 | After training loss: 858.3511352539062\n",
      "Epoch 405 - Before training loss: 858.3511352539062 | After training loss: 795.3403930664062\n",
      "\n",
      "Test set: Average loss: 5.6974, Accuracy: 3159/10000 (31.59%)\n",
      "\n",
      "Epoch 406 - Before training loss: 695.9705810546875 | After training loss: 736.8125610351562\n",
      "Epoch 407 - Before training loss: 736.8125610351562 | After training loss: 804.6541137695312\n",
      "Epoch 408 - Before training loss: 804.6541137695312 | After training loss: 762.0845336914062\n",
      "Epoch 409 - Before training loss: 762.0845336914062 | After training loss: 801.1412963867188\n",
      "Epoch 410 - Before training loss: 801.1412963867188 | After training loss: 1007.8922729492188\n",
      "\n",
      "Test set: Average loss: 7.8755, Accuracy: 2735/10000 (27.35%)\n",
      "\n",
      "Epoch 411 - Before training loss: 778.388427734375 | After training loss: 858.4163208007812\n",
      "Epoch 412 - Before training loss: 858.4163208007812 | After training loss: 808.03271484375\n",
      "Epoch 413 - Before training loss: 808.03271484375 | After training loss: 825.8604736328125\n",
      "Epoch 414 - Before training loss: 825.8604736328125 | After training loss: 822.849609375\n",
      "Epoch 415 - Before training loss: 822.849609375 | After training loss: 928.2950439453125\n",
      "\n",
      "Test set: Average loss: 5.6837, Accuracy: 3176/10000 (31.76%)\n",
      "\n",
      "Epoch 416 - Before training loss: 793.2435913085938 | After training loss: 881.9591064453125\n",
      "Epoch 417 - Before training loss: 881.9591064453125 | After training loss: 710.765380859375\n",
      "Epoch 418 - Before training loss: 710.765380859375 | After training loss: 667.2537841796875\n",
      "Epoch 419 - Before training loss: 667.2537841796875 | After training loss: 769.958740234375\n",
      "Epoch 420 - Before training loss: 769.958740234375 | After training loss: 784.59765625\n",
      "\n",
      "Test set: Average loss: 11.0579, Accuracy: 2942/10000 (29.42%)\n",
      "\n",
      "Epoch 421 - Before training loss: 711.8728637695312 | After training loss: 862.6605834960938\n",
      "Epoch 422 - Before training loss: 862.6605834960938 | After training loss: 1017.703369140625\n",
      "Epoch 423 - Before training loss: 1017.703369140625 | After training loss: 886.1768798828125\n",
      "Epoch 424 - Before training loss: 886.1768798828125 | After training loss: 859.0252075195312\n",
      "Epoch 425 - Before training loss: 859.0252075195312 | After training loss: 750.5968017578125\n",
      "\n",
      "Test set: Average loss: 4.6781, Accuracy: 3305/10000 (33.05%)\n",
      "\n",
      "Epoch 426 - Before training loss: 667.8035278320312 | After training loss: 712.5123901367188\n",
      "Epoch 427 - Before training loss: 712.5123901367188 | After training loss: 699.8322143554688\n",
      "Epoch 428 - Before training loss: 699.8322143554688 | After training loss: 761.2137451171875\n",
      "Epoch 429 - Before training loss: 761.2137451171875 | After training loss: 745.2785034179688\n",
      "Epoch 430 - Before training loss: 745.2785034179688 | After training loss: 769.9810791015625\n",
      "\n",
      "Test set: Average loss: 5.8091, Accuracy: 3167/10000 (31.67%)\n",
      "\n",
      "Epoch 431 - Before training loss: 678.5446166992188 | After training loss: 718.26953125\n",
      "Epoch 432 - Before training loss: 718.26953125 | After training loss: 711.8103637695312\n",
      "Epoch 433 - Before training loss: 711.8103637695312 | After training loss: 652.026123046875\n",
      "Epoch 434 - Before training loss: 652.026123046875 | After training loss: 674.036376953125\n",
      "Epoch 435 - Before training loss: 674.036376953125 | After training loss: 681.9949340820312\n",
      "\n",
      "Test set: Average loss: 6.4851, Accuracy: 3138/10000 (31.38%)\n",
      "\n",
      "Epoch 436 - Before training loss: 612.5769653320312 | After training loss: 811.9605712890625\n",
      "Epoch 437 - Before training loss: 811.9605712890625 | After training loss: 730.8111572265625\n",
      "Epoch 438 - Before training loss: 730.8111572265625 | After training loss: 834.0433349609375\n",
      "Epoch 439 - Before training loss: 834.0433349609375 | After training loss: 733.7516479492188\n",
      "Epoch 440 - Before training loss: 733.7516479492188 | After training loss: 674.3283081054688\n",
      "\n",
      "Test set: Average loss: 8.2960, Accuracy: 2915/10000 (29.15%)\n",
      "\n",
      "Epoch 441 - Before training loss: 614.8493041992188 | After training loss: 712.8015747070312\n",
      "Epoch 442 - Before training loss: 712.8015747070312 | After training loss: 684.8351440429688\n",
      "Epoch 443 - Before training loss: 684.8351440429688 | After training loss: 752.9432983398438\n",
      "Epoch 444 - Before training loss: 752.9432983398438 | After training loss: 773.3723754882812\n",
      "Epoch 445 - Before training loss: 773.3723754882812 | After training loss: 861.1557006835938\n",
      "\n",
      "Test set: Average loss: 8.9629, Accuracy: 2798/10000 (27.98%)\n",
      "\n",
      "Epoch 446 - Before training loss: 749.4090576171875 | After training loss: 962.513671875\n",
      "Epoch 447 - Before training loss: 962.513671875 | After training loss: 760.1957397460938\n",
      "Epoch 448 - Before training loss: 760.1957397460938 | After training loss: 621.4364013671875\n",
      "Epoch 449 - Before training loss: 621.4364013671875 | After training loss: 590.4909057617188\n",
      "Epoch 450 - Before training loss: 590.4909057617188 | After training loss: 639.1488037109375\n",
      "\n",
      "Test set: Average loss: 6.7952, Accuracy: 3124/10000 (31.24%)\n",
      "\n",
      "Epoch 451 - Before training loss: 566.4615478515625 | After training loss: 640.1572875976562\n",
      "Epoch 452 - Before training loss: 640.1572875976562 | After training loss: 707.7155151367188\n",
      "Epoch 453 - Before training loss: 707.7155151367188 | After training loss: 753.30126953125\n",
      "Epoch 454 - Before training loss: 753.30126953125 | After training loss: 855.5449829101562\n",
      "Epoch 455 - Before training loss: 855.5449829101562 | After training loss: 714.0629272460938\n",
      "\n",
      "Test set: Average loss: 7.9873, Accuracy: 2852/10000 (28.52%)\n",
      "\n",
      "Epoch 456 - Before training loss: 647.512451171875 | After training loss: 726.8338012695312\n",
      "Epoch 457 - Before training loss: 726.8338012695312 | After training loss: 705.3272705078125\n",
      "Epoch 458 - Before training loss: 705.3272705078125 | After training loss: 639.0942993164062\n",
      "Epoch 459 - Before training loss: 639.0942993164062 | After training loss: 662.5615844726562\n",
      "Epoch 460 - Before training loss: 662.5615844726562 | After training loss: 672.1627807617188\n",
      "\n",
      "Test set: Average loss: 7.0252, Accuracy: 2948/10000 (29.48%)\n",
      "\n",
      "Epoch 461 - Before training loss: 618.4835205078125 | After training loss: 811.3486938476562\n",
      "Epoch 462 - Before training loss: 811.3486938476562 | After training loss: 697.6875\n",
      "Epoch 463 - Before training loss: 697.6875 | After training loss: 760.3364868164062\n",
      "Epoch 464 - Before training loss: 760.3364868164062 | After training loss: 678.2540893554688\n",
      "Epoch 465 - Before training loss: 678.2540893554688 | After training loss: 720.4947509765625\n",
      "\n",
      "Test set: Average loss: 7.5335, Accuracy: 3146/10000 (31.46%)\n",
      "\n",
      "Epoch 466 - Before training loss: 655.556884765625 | After training loss: 703.1712036132812\n",
      "Epoch 467 - Before training loss: 703.1712036132812 | After training loss: 703.8028564453125\n",
      "Epoch 468 - Before training loss: 703.8028564453125 | After training loss: 605.455810546875\n",
      "Epoch 469 - Before training loss: 605.455810546875 | After training loss: 617.0245361328125\n",
      "Epoch 470 - Before training loss: 617.0245361328125 | After training loss: 677.13818359375\n",
      "\n",
      "Test set: Average loss: 5.3499, Accuracy: 3389/10000 (33.89%)\n",
      "\n",
      "Epoch 471 - Before training loss: 603.998291015625 | After training loss: 628.8163452148438\n",
      "Epoch 472 - Before training loss: 628.8163452148438 | After training loss: 631.400390625\n",
      "Epoch 473 - Before training loss: 631.400390625 | After training loss: 775.6038208007812\n",
      "Epoch 474 - Before training loss: 775.6038208007812 | After training loss: 780.1365356445312\n",
      "Epoch 475 - Before training loss: 780.1365356445312 | After training loss: 723.3118286132812\n",
      "\n",
      "Test set: Average loss: 6.1275, Accuracy: 3015/10000 (30.15%)\n",
      "\n",
      "Epoch 476 - Before training loss: 634.5492553710938 | After training loss: 658.5550537109375\n",
      "Epoch 477 - Before training loss: 658.5550537109375 | After training loss: 610.3172607421875\n",
      "Epoch 478 - Before training loss: 610.3172607421875 | After training loss: 626.5006103515625\n",
      "Epoch 479 - Before training loss: 626.5006103515625 | After training loss: 648.2084350585938\n",
      "Epoch 480 - Before training loss: 648.2084350585938 | After training loss: 646.7819213867188\n",
      "\n",
      "Test set: Average loss: 6.5620, Accuracy: 3154/10000 (31.54%)\n",
      "\n",
      "Epoch 481 - Before training loss: 599.7971801757812 | After training loss: 737.7601318359375\n",
      "Epoch 482 - Before training loss: 737.7601318359375 | After training loss: 769.8569946289062\n",
      "Epoch 483 - Before training loss: 769.8569946289062 | After training loss: 679.3734130859375\n",
      "Epoch 484 - Before training loss: 679.3734130859375 | After training loss: 731.715087890625\n",
      "Epoch 485 - Before training loss: 731.715087890625 | After training loss: 791.7749633789062\n",
      "\n",
      "Test set: Average loss: 7.4038, Accuracy: 2922/10000 (29.22%)\n",
      "\n",
      "Epoch 486 - Before training loss: 707.6868286132812 | After training loss: 780.8204345703125\n",
      "Epoch 487 - Before training loss: 780.8204345703125 | After training loss: 650.9483642578125\n",
      "Epoch 488 - Before training loss: 650.9483642578125 | After training loss: 602.9928588867188\n",
      "Epoch 489 - Before training loss: 602.9928588867188 | After training loss: 653.3634033203125\n",
      "Epoch 490 - Before training loss: 653.3634033203125 | After training loss: 686.7461547851562\n",
      "\n",
      "Test set: Average loss: 8.6790, Accuracy: 3003/10000 (30.03%)\n",
      "\n",
      "Epoch 491 - Before training loss: 624.0718994140625 | After training loss: 697.886962890625\n",
      "Epoch 492 - Before training loss: 697.886962890625 | After training loss: 664.1217651367188\n",
      "Epoch 493 - Before training loss: 664.1217651367188 | After training loss: 676.5716552734375\n",
      "Epoch 494 - Before training loss: 676.5716552734375 | After training loss: 620.0684204101562\n",
      "Epoch 495 - Before training loss: 620.0684204101562 | After training loss: 759.2208862304688\n",
      "\n",
      "Test set: Average loss: 10.4844, Accuracy: 2906/10000 (29.06%)\n",
      "\n",
      "Epoch 496 - Before training loss: 671.2080688476562 | After training loss: 1113.682373046875\n",
      "Epoch 497 - Before training loss: 1113.682373046875 | After training loss: 1200.319580078125\n",
      "Epoch 498 - Before training loss: 1200.319580078125 | After training loss: 693.017333984375\n",
      "Epoch 499 - Before training loss: 693.017333984375 | After training loss: 561.1842041015625\n",
      "Epoch 500 - Before training loss: 561.1842041015625 | After training loss: 557.2348022460938\n",
      "\n",
      "Test set: Average loss: 5.6339, Accuracy: 3226/10000 (32.26%)\n",
      "\n",
      "Epoch 501 - Before training loss: 523.5148315429688 | After training loss: 562.8698120117188\n",
      "Epoch 502 - Before training loss: 562.8698120117188 | After training loss: 556.2730712890625\n",
      "Epoch 503 - Before training loss: 556.2730712890625 | After training loss: 540.021728515625\n",
      "Epoch 504 - Before training loss: 540.021728515625 | After training loss: 528.4373168945312\n",
      "Epoch 505 - Before training loss: 528.4373168945312 | After training loss: 661.302490234375\n",
      "\n",
      "Test set: Average loss: 8.0550, Accuracy: 3200/10000 (32.00%)\n",
      "\n",
      "Epoch 506 - Before training loss: 589.841796875 | After training loss: 662.125244140625\n",
      "Epoch 507 - Before training loss: 662.125244140625 | After training loss: 941.572265625\n",
      "Epoch 508 - Before training loss: 941.572265625 | After training loss: 893.9334106445312\n",
      "Epoch 509 - Before training loss: 893.9334106445312 | After training loss: 738.5687255859375\n",
      "Epoch 510 - Before training loss: 738.5687255859375 | After training loss: 590.324951171875\n",
      "\n",
      "Test set: Average loss: 6.6416, Accuracy: 3377/10000 (33.77%)\n",
      "\n",
      "Epoch 511 - Before training loss: 546.4135131835938 | After training loss: 573.1663208007812\n",
      "Epoch 512 - Before training loss: 573.1663208007812 | After training loss: 502.5832214355469\n",
      "Epoch 513 - Before training loss: 502.5832214355469 | After training loss: 535.8171997070312\n",
      "Epoch 514 - Before training loss: 535.8171997070312 | After training loss: 574.5453491210938\n",
      "Epoch 515 - Before training loss: 574.5453491210938 | After training loss: 630.9933471679688\n",
      "\n",
      "Test set: Average loss: 5.3483, Accuracy: 3350/10000 (33.50%)\n",
      "\n",
      "Epoch 516 - Before training loss: 592.0308837890625 | After training loss: 634.5648803710938\n",
      "Epoch 517 - Before training loss: 634.5648803710938 | After training loss: 658.1973876953125\n",
      "Epoch 518 - Before training loss: 658.1973876953125 | After training loss: 650.9442749023438\n",
      "Epoch 519 - Before training loss: 650.9442749023438 | After training loss: 713.7566528320312\n",
      "Epoch 520 - Before training loss: 713.7566528320312 | After training loss: 634.9597778320312\n",
      "\n",
      "Test set: Average loss: 8.7562, Accuracy: 2786/10000 (27.86%)\n",
      "\n",
      "Epoch 521 - Before training loss: 594.4659423828125 | After training loss: 612.355712890625\n",
      "Epoch 522 - Before training loss: 612.355712890625 | After training loss: 728.1983642578125\n",
      "Epoch 523 - Before training loss: 728.1983642578125 | After training loss: 876.3367309570312\n",
      "Epoch 524 - Before training loss: 876.3367309570312 | After training loss: 753.3480224609375\n",
      "Epoch 525 - Before training loss: 753.3480224609375 | After training loss: 679.9972534179688\n",
      "\n",
      "Test set: Average loss: 8.0099, Accuracy: 2935/10000 (29.35%)\n",
      "\n",
      "Epoch 526 - Before training loss: 584.1483154296875 | After training loss: 623.38623046875\n",
      "Epoch 527 - Before training loss: 623.38623046875 | After training loss: 531.9539794921875\n",
      "Epoch 528 - Before training loss: 531.9539794921875 | After training loss: 506.4179382324219\n",
      "Epoch 529 - Before training loss: 506.4179382324219 | After training loss: 517.3499145507812\n",
      "Epoch 530 - Before training loss: 517.3499145507812 | After training loss: 549.8038940429688\n",
      "\n",
      "Test set: Average loss: 5.8182, Accuracy: 3222/10000 (32.22%)\n",
      "\n",
      "Epoch 531 - Before training loss: 515.2955322265625 | After training loss: 610.715087890625\n",
      "Epoch 532 - Before training loss: 610.715087890625 | After training loss: 629.3733520507812\n",
      "Epoch 533 - Before training loss: 629.3733520507812 | After training loss: 633.3770751953125\n",
      "Epoch 534 - Before training loss: 633.3770751953125 | After training loss: 597.7947998046875\n",
      "Epoch 535 - Before training loss: 597.7947998046875 | After training loss: 674.8899536132812\n",
      "\n",
      "Test set: Average loss: 7.0706, Accuracy: 3052/10000 (30.52%)\n",
      "\n",
      "Epoch 536 - Before training loss: 584.1605834960938 | After training loss: 588.0595703125\n",
      "Epoch 537 - Before training loss: 588.0595703125 | After training loss: 716.80029296875\n",
      "Epoch 538 - Before training loss: 716.80029296875 | After training loss: 680.1707763671875\n",
      "Epoch 539 - Before training loss: 680.1707763671875 | After training loss: 651.2512817382812\n",
      "Epoch 540 - Before training loss: 651.2512817382812 | After training loss: 558.0865478515625\n",
      "\n",
      "Test set: Average loss: 5.7670, Accuracy: 3075/10000 (30.75%)\n",
      "\n",
      "Epoch 541 - Before training loss: 510.511474609375 | After training loss: 555.1704711914062\n",
      "Epoch 542 - Before training loss: 555.1704711914062 | After training loss: 507.9461669921875\n",
      "Epoch 543 - Before training loss: 507.9461669921875 | After training loss: 532.1044921875\n",
      "Epoch 544 - Before training loss: 532.1044921875 | After training loss: 509.1044616699219\n",
      "Epoch 545 - Before training loss: 509.1044616699219 | After training loss: 598.9086303710938\n",
      "\n",
      "Test set: Average loss: 10.0302, Accuracy: 3000/10000 (30.00%)\n",
      "\n",
      "Epoch 546 - Before training loss: 539.1281127929688 | After training loss: 574.736083984375\n",
      "Epoch 547 - Before training loss: 574.736083984375 | After training loss: 661.94140625\n",
      "Epoch 548 - Before training loss: 661.94140625 | After training loss: 649.9812622070312\n",
      "Epoch 549 - Before training loss: 649.9812622070312 | After training loss: 677.766845703125\n",
      "Epoch 550 - Before training loss: 677.766845703125 | After training loss: 570.7815551757812\n",
      "\n",
      "Test set: Average loss: 8.1328, Accuracy: 3168/10000 (31.68%)\n",
      "\n",
      "Epoch 551 - Before training loss: 518.9877319335938 | After training loss: 589.6275024414062\n",
      "Epoch 552 - Before training loss: 589.6275024414062 | After training loss: 657.7318725585938\n",
      "Epoch 553 - Before training loss: 657.7318725585938 | After training loss: 706.4869995117188\n",
      "Epoch 554 - Before training loss: 706.4869995117188 | After training loss: 635.5394897460938\n",
      "Epoch 555 - Before training loss: 635.5394897460938 | After training loss: 564.1599731445312\n",
      "\n",
      "Test set: Average loss: 5.4772, Accuracy: 3363/10000 (33.63%)\n",
      "\n",
      "Epoch 556 - Before training loss: 504.50115966796875 | After training loss: 517.7339477539062\n",
      "Epoch 557 - Before training loss: 517.7339477539062 | After training loss: 526.47705078125\n",
      "Epoch 558 - Before training loss: 526.47705078125 | After training loss: 517.9127197265625\n",
      "Epoch 559 - Before training loss: 517.9127197265625 | After training loss: 565.5846557617188\n",
      "Epoch 560 - Before training loss: 565.5846557617188 | After training loss: 564.444091796875\n",
      "\n",
      "Test set: Average loss: 7.3357, Accuracy: 3040/10000 (30.40%)\n",
      "\n",
      "Epoch 561 - Before training loss: 525.45849609375 | After training loss: 571.3637084960938\n",
      "Epoch 562 - Before training loss: 571.3637084960938 | After training loss: 639.397705078125\n",
      "Epoch 563 - Before training loss: 639.397705078125 | After training loss: 696.4684448242188\n",
      "Epoch 564 - Before training loss: 696.4684448242188 | After training loss: 611.3153686523438\n",
      "Epoch 565 - Before training loss: 611.3153686523438 | After training loss: 536.429443359375\n",
      "\n",
      "Test set: Average loss: 5.9985, Accuracy: 3029/10000 (30.29%)\n",
      "\n",
      "Epoch 566 - Before training loss: 498.5197448730469 | After training loss: 531.4434814453125\n",
      "Epoch 567 - Before training loss: 531.4434814453125 | After training loss: 581.7164916992188\n",
      "Epoch 568 - Before training loss: 581.7164916992188 | After training loss: 516.2190551757812\n",
      "Epoch 569 - Before training loss: 516.2190551757812 | After training loss: 576.3924560546875\n",
      "Epoch 570 - Before training loss: 576.3924560546875 | After training loss: 597.2202758789062\n",
      "\n",
      "Test set: Average loss: 8.9861, Accuracy: 2865/10000 (28.65%)\n",
      "\n",
      "Epoch 571 - Before training loss: 516.3994140625 | After training loss: 595.3463745117188\n",
      "Epoch 572 - Before training loss: 595.3463745117188 | After training loss: 541.1837158203125\n",
      "Epoch 573 - Before training loss: 541.1837158203125 | After training loss: 573.381591796875\n",
      "Epoch 574 - Before training loss: 573.381591796875 | After training loss: 527.445556640625\n",
      "Epoch 575 - Before training loss: 527.445556640625 | After training loss: 556.9061889648438\n",
      "\n",
      "Test set: Average loss: 8.9288, Accuracy: 3026/10000 (30.26%)\n",
      "\n",
      "Epoch 576 - Before training loss: 520.5707397460938 | After training loss: 524.4706420898438\n",
      "Epoch 577 - Before training loss: 524.4706420898438 | After training loss: 584.0604248046875\n",
      "Epoch 578 - Before training loss: 584.0604248046875 | After training loss: 560.6605834960938\n",
      "Epoch 579 - Before training loss: 560.6605834960938 | After training loss: 695.9757080078125\n",
      "Epoch 580 - Before training loss: 695.9757080078125 | After training loss: 560.8504638671875\n",
      "\n",
      "Test set: Average loss: 6.8284, Accuracy: 3076/10000 (30.76%)\n",
      "\n",
      "Epoch 581 - Before training loss: 517.4572143554688 | After training loss: 548.6337890625\n",
      "Epoch 582 - Before training loss: 548.6337890625 | After training loss: 470.9565124511719\n",
      "Epoch 583 - Before training loss: 470.9565124511719 | After training loss: 511.06292724609375\n",
      "Epoch 584 - Before training loss: 511.06292724609375 | After training loss: 497.3482360839844\n",
      "Epoch 585 - Before training loss: 497.3482360839844 | After training loss: 607.6661376953125\n",
      "\n",
      "Test set: Average loss: 6.5985, Accuracy: 3043/10000 (30.43%)\n",
      "\n",
      "Epoch 586 - Before training loss: 539.6554565429688 | After training loss: 611.2967529296875\n",
      "Epoch 587 - Before training loss: 611.2967529296875 | After training loss: 565.1486206054688\n",
      "Epoch 588 - Before training loss: 565.1486206054688 | After training loss: 557.2123413085938\n",
      "Epoch 589 - Before training loss: 557.2123413085938 | After training loss: 564.3845825195312\n",
      "Epoch 590 - Before training loss: 564.3845825195312 | After training loss: 483.3234558105469\n",
      "\n",
      "Test set: Average loss: 6.0701, Accuracy: 3112/10000 (31.12%)\n",
      "\n",
      "Epoch 591 - Before training loss: 456.4515380859375 | After training loss: 472.9263916015625\n",
      "Epoch 592 - Before training loss: 472.9263916015625 | After training loss: 437.657470703125\n",
      "Epoch 593 - Before training loss: 437.657470703125 | After training loss: 481.1931457519531\n",
      "Epoch 594 - Before training loss: 481.1931457519531 | After training loss: 468.11212158203125\n",
      "Epoch 595 - Before training loss: 468.11212158203125 | After training loss: 557.9739990234375\n",
      "\n",
      "Test set: Average loss: 5.6027, Accuracy: 3137/10000 (31.37%)\n",
      "\n",
      "Epoch 596 - Before training loss: 496.4537048339844 | After training loss: 557.4862670898438\n",
      "Epoch 597 - Before training loss: 557.4862670898438 | After training loss: 610.786865234375\n",
      "Epoch 598 - Before training loss: 610.786865234375 | After training loss: 782.7879028320312\n",
      "Epoch 599 - Before training loss: 782.7879028320312 | After training loss: 658.5772705078125\n",
      "Epoch 600 - Before training loss: 658.5772705078125 | After training loss: 548.465087890625\n",
      "\n",
      "Test set: Average loss: 6.6088, Accuracy: 3305/10000 (33.05%)\n",
      "\n",
      "Epoch 601 - Before training loss: 488.1372375488281 | After training loss: 515.9566040039062\n",
      "Epoch 602 - Before training loss: 515.9566040039062 | After training loss: 498.40130615234375\n",
      "Epoch 603 - Before training loss: 498.40130615234375 | After training loss: 534.8550415039062\n",
      "Epoch 604 - Before training loss: 534.8550415039062 | After training loss: 491.0206604003906\n",
      "Epoch 605 - Before training loss: 491.0206604003906 | After training loss: 501.7660217285156\n",
      "\n",
      "Test set: Average loss: 6.5543, Accuracy: 3107/10000 (31.07%)\n",
      "\n",
      "Epoch 606 - Before training loss: 459.77069091796875 | After training loss: 486.1822814941406\n",
      "Epoch 607 - Before training loss: 486.1822814941406 | After training loss: 508.9400634765625\n",
      "Epoch 608 - Before training loss: 508.9400634765625 | After training loss: 556.3739013671875\n",
      "Epoch 609 - Before training loss: 556.3739013671875 | After training loss: 610.7089233398438\n",
      "Epoch 610 - Before training loss: 610.7089233398438 | After training loss: 620.6500244140625\n",
      "\n",
      "Test set: Average loss: 7.5441, Accuracy: 2905/10000 (29.05%)\n",
      "\n",
      "Epoch 611 - Before training loss: 547.58740234375 | After training loss: 573.07666015625\n",
      "Epoch 612 - Before training loss: 573.07666015625 | After training loss: 515.5603637695312\n",
      "Epoch 613 - Before training loss: 515.5603637695312 | After training loss: 498.0688781738281\n",
      "Epoch 614 - Before training loss: 498.0688781738281 | After training loss: 452.11102294921875\n",
      "Epoch 615 - Before training loss: 452.11102294921875 | After training loss: 557.2996826171875\n",
      "\n",
      "Test set: Average loss: 8.7794, Accuracy: 2952/10000 (29.52%)\n",
      "\n",
      "Epoch 616 - Before training loss: 493.39208984375 | After training loss: 487.79022216796875\n",
      "Epoch 617 - Before training loss: 487.79022216796875 | After training loss: 504.01568603515625\n",
      "Epoch 618 - Before training loss: 504.01568603515625 | After training loss: 618.2144775390625\n",
      "Epoch 619 - Before training loss: 618.2144775390625 | After training loss: 633.61083984375\n",
      "Epoch 620 - Before training loss: 633.61083984375 | After training loss: 594.7030029296875\n",
      "\n",
      "Test set: Average loss: 5.8025, Accuracy: 2950/10000 (29.50%)\n",
      "\n",
      "Epoch 621 - Before training loss: 516.8367919921875 | After training loss: 625.5125122070312\n",
      "Epoch 622 - Before training loss: 625.5125122070312 | After training loss: 569.734130859375\n",
      "Epoch 623 - Before training loss: 569.734130859375 | After training loss: 498.4849853515625\n",
      "Epoch 624 - Before training loss: 498.4849853515625 | After training loss: 465.5170593261719\n",
      "Epoch 625 - Before training loss: 465.5170593261719 | After training loss: 458.63568115234375\n",
      "\n",
      "Test set: Average loss: 6.6046, Accuracy: 2962/10000 (29.62%)\n",
      "\n",
      "Epoch 626 - Before training loss: 430.13836669921875 | After training loss: 461.3485412597656\n",
      "Epoch 627 - Before training loss: 461.3485412597656 | After training loss: 517.3269653320312\n",
      "Epoch 628 - Before training loss: 517.3269653320312 | After training loss: 452.281005859375\n",
      "Epoch 629 - Before training loss: 452.281005859375 | After training loss: 469.1496887207031\n",
      "Epoch 630 - Before training loss: 469.1496887207031 | After training loss: 486.2296142578125\n",
      "\n",
      "Test set: Average loss: 8.1373, Accuracy: 3059/10000 (30.59%)\n",
      "\n",
      "Epoch 631 - Before training loss: 445.9409484863281 | After training loss: 496.8113708496094\n",
      "Epoch 632 - Before training loss: 496.8113708496094 | After training loss: 467.5745544433594\n",
      "Epoch 633 - Before training loss: 467.5745544433594 | After training loss: 485.87823486328125\n",
      "Epoch 634 - Before training loss: 485.87823486328125 | After training loss: 558.2958984375\n",
      "Epoch 635 - Before training loss: 558.2958984375 | After training loss: 529.990966796875\n",
      "\n",
      "Test set: Average loss: 6.6016, Accuracy: 3256/10000 (32.56%)\n",
      "\n",
      "Epoch 636 - Before training loss: 496.4419860839844 | After training loss: 604.3045654296875\n",
      "Epoch 637 - Before training loss: 604.3045654296875 | After training loss: 720.4429931640625\n",
      "Epoch 638 - Before training loss: 720.4429931640625 | After training loss: 529.2131958007812\n",
      "Epoch 639 - Before training loss: 529.2131958007812 | After training loss: 501.8411560058594\n",
      "Epoch 640 - Before training loss: 501.8411560058594 | After training loss: 551.0357055664062\n",
      "\n",
      "Test set: Average loss: 5.7906, Accuracy: 3218/10000 (32.18%)\n",
      "\n",
      "Epoch 641 - Before training loss: 483.419921875 | After training loss: 638.928955078125\n",
      "Epoch 642 - Before training loss: 638.928955078125 | After training loss: 539.1581420898438\n",
      "Epoch 643 - Before training loss: 539.1581420898438 | After training loss: 459.66583251953125\n",
      "Epoch 644 - Before training loss: 459.66583251953125 | After training loss: 479.9780578613281\n",
      "Epoch 645 - Before training loss: 479.9780578613281 | After training loss: 473.80548095703125\n",
      "\n",
      "Test set: Average loss: 5.3959, Accuracy: 3152/10000 (31.52%)\n",
      "\n",
      "Epoch 646 - Before training loss: 445.2967834472656 | After training loss: 500.6701965332031\n",
      "Epoch 647 - Before training loss: 500.6701965332031 | After training loss: 491.9808654785156\n",
      "Epoch 648 - Before training loss: 491.9808654785156 | After training loss: 577.2437744140625\n",
      "Epoch 649 - Before training loss: 577.2437744140625 | After training loss: 595.279052734375\n",
      "Epoch 650 - Before training loss: 595.279052734375 | After training loss: 531.3150634765625\n",
      "\n",
      "Test set: Average loss: 10.0815, Accuracy: 2719/10000 (27.19%)\n",
      "\n",
      "Epoch 651 - Before training loss: 478.8357238769531 | After training loss: 615.0945434570312\n",
      "Epoch 652 - Before training loss: 615.0945434570312 | After training loss: 497.5329895019531\n",
      "Epoch 653 - Before training loss: 497.5329895019531 | After training loss: 475.97369384765625\n",
      "Epoch 654 - Before training loss: 475.97369384765625 | After training loss: 458.7650451660156\n",
      "Epoch 655 - Before training loss: 458.7650451660156 | After training loss: 499.8726806640625\n",
      "\n",
      "Test set: Average loss: 6.4649, Accuracy: 3044/10000 (30.44%)\n",
      "\n",
      "Epoch 656 - Before training loss: 454.59674072265625 | After training loss: 465.11627197265625\n",
      "Epoch 657 - Before training loss: 465.11627197265625 | After training loss: 621.4744873046875\n",
      "Epoch 658 - Before training loss: 621.4744873046875 | After training loss: 628.8982543945312\n",
      "Epoch 659 - Before training loss: 628.8982543945312 | After training loss: 577.2283935546875\n",
      "Epoch 660 - Before training loss: 577.2283935546875 | After training loss: 458.06884765625\n",
      "\n",
      "Test set: Average loss: 7.0413, Accuracy: 3044/10000 (30.44%)\n",
      "\n",
      "Epoch 661 - Before training loss: 426.8805847167969 | After training loss: 463.4871826171875\n",
      "Epoch 662 - Before training loss: 463.4871826171875 | After training loss: 462.4549560546875\n",
      "Epoch 663 - Before training loss: 462.4549560546875 | After training loss: 484.0496826171875\n",
      "Epoch 664 - Before training loss: 484.0496826171875 | After training loss: 439.91668701171875\n",
      "Epoch 665 - Before training loss: 439.91668701171875 | After training loss: 528.6085205078125\n",
      "\n",
      "Test set: Average loss: 8.4067, Accuracy: 2982/10000 (29.82%)\n",
      "\n",
      "Epoch 666 - Before training loss: 482.16217041015625 | After training loss: 484.8971862792969\n",
      "Epoch 667 - Before training loss: 484.8971862792969 | After training loss: 482.5134582519531\n",
      "Epoch 668 - Before training loss: 482.5134582519531 | After training loss: 486.9169921875\n",
      "Epoch 669 - Before training loss: 486.9169921875 | After training loss: 560.7344360351562\n",
      "Epoch 670 - Before training loss: 560.7344360351562 | After training loss: 496.08984375\n",
      "\n",
      "Test set: Average loss: 6.4585, Accuracy: 3076/10000 (30.76%)\n",
      "\n",
      "Epoch 671 - Before training loss: 469.4544677734375 | After training loss: 546.1692504882812\n",
      "Epoch 672 - Before training loss: 546.1692504882812 | After training loss: 462.9178771972656\n",
      "Epoch 673 - Before training loss: 462.9178771972656 | After training loss: 501.1664123535156\n",
      "Epoch 674 - Before training loss: 501.1664123535156 | After training loss: 437.5317077636719\n",
      "Epoch 675 - Before training loss: 437.5317077636719 | After training loss: 467.4052734375\n",
      "\n",
      "Test set: Average loss: 8.0937, Accuracy: 3103/10000 (31.03%)\n",
      "\n",
      "Epoch 676 - Before training loss: 430.36260986328125 | After training loss: 470.3154296875\n",
      "Epoch 677 - Before training loss: 470.3154296875 | After training loss: 473.537109375\n",
      "Epoch 678 - Before training loss: 473.537109375 | After training loss: 569.3225708007812\n",
      "Epoch 679 - Before training loss: 569.3225708007812 | After training loss: 560.1602783203125\n",
      "Epoch 680 - Before training loss: 560.1602783203125 | After training loss: 613.1836547851562\n",
      "\n",
      "Test set: Average loss: 7.4329, Accuracy: 3108/10000 (31.08%)\n",
      "\n",
      "Epoch 681 - Before training loss: 529.2837524414062 | After training loss: 563.8783569335938\n",
      "Epoch 682 - Before training loss: 563.8783569335938 | After training loss: 534.81005859375\n",
      "Epoch 683 - Before training loss: 534.81005859375 | After training loss: 519.8023071289062\n",
      "Epoch 684 - Before training loss: 519.8023071289062 | After training loss: 542.347412109375\n",
      "Epoch 685 - Before training loss: 542.347412109375 | After training loss: 476.1703186035156\n",
      "\n",
      "Test set: Average loss: 9.2128, Accuracy: 2768/10000 (27.68%)\n",
      "\n",
      "Epoch 686 - Before training loss: 458.10955810546875 | After training loss: 463.41107177734375\n",
      "Epoch 687 - Before training loss: 463.41107177734375 | After training loss: 447.0281677246094\n",
      "Epoch 688 - Before training loss: 447.0281677246094 | After training loss: 458.0301513671875\n",
      "Epoch 689 - Before training loss: 458.0301513671875 | After training loss: 426.63909912109375\n",
      "Epoch 690 - Before training loss: 426.63909912109375 | After training loss: 455.3812561035156\n",
      "\n",
      "Test set: Average loss: 8.4206, Accuracy: 3162/10000 (31.62%)\n",
      "\n",
      "Epoch 691 - Before training loss: 415.42828369140625 | After training loss: 412.1371154785156\n",
      "Epoch 692 - Before training loss: 412.1371154785156 | After training loss: 430.9638977050781\n",
      "Epoch 693 - Before training loss: 430.9638977050781 | After training loss: 449.88665771484375\n",
      "Epoch 694 - Before training loss: 449.88665771484375 | After training loss: 485.4150695800781\n",
      "Epoch 695 - Before training loss: 485.4150695800781 | After training loss: 522.3065185546875\n",
      "\n",
      "Test set: Average loss: 6.1780, Accuracy: 3123/10000 (31.23%)\n",
      "\n",
      "Epoch 696 - Before training loss: 491.30450439453125 | After training loss: 539.21337890625\n",
      "Epoch 697 - Before training loss: 539.21337890625 | After training loss: 518.2054443359375\n",
      "Epoch 698 - Before training loss: 518.2054443359375 | After training loss: 510.7999572753906\n",
      "Epoch 699 - Before training loss: 510.7999572753906 | After training loss: 436.70733642578125\n",
      "Epoch 700 - Before training loss: 436.70733642578125 | After training loss: 485.26483154296875\n",
      "\n",
      "Test set: Average loss: 6.5330, Accuracy: 3189/10000 (31.89%)\n",
      "\n",
      "Epoch 701 - Before training loss: 427.27325439453125 | After training loss: 424.7228698730469\n",
      "Epoch 702 - Before training loss: 424.7228698730469 | After training loss: 390.5084228515625\n",
      "Epoch 703 - Before training loss: 390.5084228515625 | After training loss: 395.8840026855469\n",
      "Epoch 704 - Before training loss: 395.8840026855469 | After training loss: 386.0874938964844\n",
      "Epoch 705 - Before training loss: 386.0874938964844 | After training loss: 415.5514221191406\n",
      "\n",
      "Test set: Average loss: 6.9690, Accuracy: 2993/10000 (29.93%)\n",
      "\n",
      "Epoch 706 - Before training loss: 390.8736877441406 | After training loss: 486.460693359375\n",
      "Epoch 707 - Before training loss: 486.460693359375 | After training loss: 445.5987243652344\n",
      "Epoch 708 - Before training loss: 445.5987243652344 | After training loss: 483.4344787597656\n",
      "Epoch 709 - Before training loss: 483.4344787597656 | After training loss: 539.5047607421875\n",
      "Epoch 710 - Before training loss: 539.5047607421875 | After training loss: 590.6656494140625\n",
      "\n",
      "Test set: Average loss: 7.0639, Accuracy: 2969/10000 (29.69%)\n",
      "\n",
      "Epoch 711 - Before training loss: 522.09423828125 | After training loss: 574.0355224609375\n",
      "Epoch 712 - Before training loss: 574.0355224609375 | After training loss: 568.097900390625\n",
      "Epoch 713 - Before training loss: 568.097900390625 | After training loss: 462.3531188964844\n",
      "Epoch 714 - Before training loss: 462.3531188964844 | After training loss: 473.88458251953125\n",
      "Epoch 715 - Before training loss: 473.88458251953125 | After training loss: 423.3462829589844\n",
      "\n",
      "Test set: Average loss: 5.8746, Accuracy: 3333/10000 (33.33%)\n",
      "\n",
      "Epoch 716 - Before training loss: 385.86187744140625 | After training loss: 406.558349609375\n",
      "Epoch 717 - Before training loss: 406.558349609375 | After training loss: 394.2565612792969\n",
      "Epoch 718 - Before training loss: 394.2565612792969 | After training loss: 412.4296875\n",
      "Epoch 719 - Before training loss: 412.4296875 | After training loss: 452.6058654785156\n",
      "Epoch 720 - Before training loss: 452.6058654785156 | After training loss: 458.4157409667969\n",
      "\n",
      "Test set: Average loss: 7.0882, Accuracy: 2831/10000 (28.31%)\n",
      "\n",
      "Epoch 721 - Before training loss: 406.72308349609375 | After training loss: 436.4710998535156\n",
      "Epoch 722 - Before training loss: 436.4710998535156 | After training loss: 482.237060546875\n",
      "Epoch 723 - Before training loss: 482.237060546875 | After training loss: 466.5739440917969\n",
      "Epoch 724 - Before training loss: 466.5739440917969 | After training loss: 477.7995910644531\n",
      "Epoch 725 - Before training loss: 477.7995910644531 | After training loss: 523.6672973632812\n",
      "\n",
      "Test set: Average loss: 6.7434, Accuracy: 2977/10000 (29.77%)\n",
      "\n",
      "Epoch 726 - Before training loss: 466.1674499511719 | After training loss: 524.2390747070312\n",
      "Epoch 727 - Before training loss: 524.2390747070312 | After training loss: 450.71881103515625\n",
      "Epoch 728 - Before training loss: 450.71881103515625 | After training loss: 524.9949951171875\n",
      "Epoch 729 - Before training loss: 524.9949951171875 | After training loss: 525.0410766601562\n",
      "Epoch 730 - Before training loss: 525.0410766601562 | After training loss: 526.11328125\n",
      "\n",
      "Test set: Average loss: 7.9309, Accuracy: 3010/10000 (30.10%)\n",
      "\n",
      "Epoch 731 - Before training loss: 480.02923583984375 | After training loss: 458.5049133300781\n",
      "Epoch 732 - Before training loss: 458.5049133300781 | After training loss: 425.3109130859375\n",
      "Epoch 733 - Before training loss: 425.3109130859375 | After training loss: 442.14447021484375\n",
      "Epoch 734 - Before training loss: 442.14447021484375 | After training loss: 448.93109130859375\n",
      "Epoch 735 - Before training loss: 448.93109130859375 | After training loss: 445.7952880859375\n",
      "\n",
      "Test set: Average loss: 6.6455, Accuracy: 3102/10000 (31.02%)\n",
      "\n",
      "Epoch 736 - Before training loss: 396.76025390625 | After training loss: 453.9510192871094\n",
      "Epoch 737 - Before training loss: 453.9510192871094 | After training loss: 523.8305053710938\n",
      "Epoch 738 - Before training loss: 523.8305053710938 | After training loss: 499.23626708984375\n",
      "Epoch 739 - Before training loss: 499.23626708984375 | After training loss: 431.13568115234375\n",
      "Epoch 740 - Before training loss: 431.13568115234375 | After training loss: 458.5127868652344\n",
      "\n",
      "Test set: Average loss: 6.0407, Accuracy: 3146/10000 (31.46%)\n",
      "\n",
      "Epoch 741 - Before training loss: 424.60345458984375 | After training loss: 541.6862182617188\n",
      "Epoch 742 - Before training loss: 541.6862182617188 | After training loss: 591.2841796875\n",
      "Epoch 743 - Before training loss: 591.2841796875 | After training loss: 469.8327331542969\n",
      "Epoch 744 - Before training loss: 469.8327331542969 | After training loss: 481.0351257324219\n",
      "Epoch 745 - Before training loss: 481.0351257324219 | After training loss: 446.9523010253906\n",
      "\n",
      "Test set: Average loss: 9.7394, Accuracy: 2844/10000 (28.44%)\n",
      "\n",
      "Epoch 746 - Before training loss: 422.0985107421875 | After training loss: 440.1978454589844\n",
      "Epoch 747 - Before training loss: 440.1978454589844 | After training loss: 419.6559753417969\n",
      "Epoch 748 - Before training loss: 419.6559753417969 | After training loss: 421.9319763183594\n",
      "Epoch 749 - Before training loss: 421.9319763183594 | After training loss: 437.1297607421875\n",
      "Epoch 750 - Before training loss: 437.1297607421875 | After training loss: 429.9322814941406\n",
      "\n",
      "Test set: Average loss: 6.4800, Accuracy: 3013/10000 (30.13%)\n",
      "\n",
      "Epoch 751 - Before training loss: 404.806884765625 | After training loss: 398.3088073730469\n",
      "Epoch 752 - Before training loss: 398.3088073730469 | After training loss: 433.3404846191406\n",
      "Epoch 753 - Before training loss: 433.3404846191406 | After training loss: 446.21044921875\n",
      "Epoch 754 - Before training loss: 446.21044921875 | After training loss: 485.7503356933594\n",
      "Epoch 755 - Before training loss: 485.7503356933594 | After training loss: 418.3418884277344\n",
      "\n",
      "Test set: Average loss: 6.1742, Accuracy: 3273/10000 (32.73%)\n",
      "\n",
      "Epoch 756 - Before training loss: 392.5166320800781 | After training loss: 472.8392333984375\n",
      "Epoch 757 - Before training loss: 472.8392333984375 | After training loss: 449.392333984375\n",
      "Epoch 758 - Before training loss: 449.392333984375 | After training loss: 537.35595703125\n",
      "Epoch 759 - Before training loss: 537.35595703125 | After training loss: 507.4080810546875\n",
      "Epoch 760 - Before training loss: 507.4080810546875 | After training loss: 502.4668273925781\n",
      "\n",
      "Test set: Average loss: 7.6315, Accuracy: 2982/10000 (29.82%)\n",
      "\n",
      "Epoch 761 - Before training loss: 432.9247131347656 | After training loss: 400.91864013671875\n",
      "Epoch 762 - Before training loss: 400.91864013671875 | After training loss: 403.3919677734375\n",
      "Epoch 763 - Before training loss: 403.3919677734375 | After training loss: 379.4876403808594\n",
      "Epoch 764 - Before training loss: 379.4876403808594 | After training loss: 414.49969482421875\n",
      "Epoch 765 - Before training loss: 414.49969482421875 | After training loss: 420.73565673828125\n",
      "\n",
      "Test set: Average loss: 8.9046, Accuracy: 2897/10000 (28.97%)\n",
      "\n",
      "Epoch 766 - Before training loss: 382.4391174316406 | After training loss: 458.68389892578125\n",
      "Epoch 767 - Before training loss: 458.68389892578125 | After training loss: 452.15594482421875\n",
      "Epoch 768 - Before training loss: 452.15594482421875 | After training loss: 458.86871337890625\n",
      "Epoch 769 - Before training loss: 458.86871337890625 | After training loss: 414.9686584472656\n",
      "Epoch 770 - Before training loss: 414.9686584472656 | After training loss: 480.69451904296875\n",
      "\n",
      "Test set: Average loss: 5.1850, Accuracy: 3138/10000 (31.38%)\n",
      "\n",
      "Epoch 771 - Before training loss: 433.1057434082031 | After training loss: 502.390380859375\n",
      "Epoch 772 - Before training loss: 502.390380859375 | After training loss: 516.5598754882812\n",
      "Epoch 773 - Before training loss: 516.5598754882812 | After training loss: 468.3475646972656\n",
      "Epoch 774 - Before training loss: 468.3475646972656 | After training loss: 506.9216003417969\n",
      "Epoch 775 - Before training loss: 506.9216003417969 | After training loss: 405.5370178222656\n",
      "\n",
      "Test set: Average loss: 5.9976, Accuracy: 3136/10000 (31.36%)\n",
      "\n",
      "Epoch 776 - Before training loss: 378.7686462402344 | After training loss: 403.369384765625\n",
      "Epoch 777 - Before training loss: 403.369384765625 | After training loss: 418.2561340332031\n",
      "Epoch 778 - Before training loss: 418.2561340332031 | After training loss: 448.4575500488281\n",
      "Epoch 779 - Before training loss: 448.4575500488281 | After training loss: 425.6159362792969\n",
      "Epoch 780 - Before training loss: 425.6159362792969 | After training loss: 436.9752197265625\n",
      "\n",
      "Test set: Average loss: 5.6810, Accuracy: 3222/10000 (32.22%)\n",
      "\n",
      "Epoch 781 - Before training loss: 411.05255126953125 | After training loss: 430.67462158203125\n",
      "Epoch 782 - Before training loss: 430.67462158203125 | After training loss: 408.75384521484375\n",
      "Epoch 783 - Before training loss: 408.75384521484375 | After training loss: 407.1857604980469\n",
      "Epoch 784 - Before training loss: 407.1857604980469 | After training loss: 416.79840087890625\n",
      "Epoch 785 - Before training loss: 416.79840087890625 | After training loss: 388.1874694824219\n",
      "\n",
      "Test set: Average loss: 7.6714, Accuracy: 2937/10000 (29.37%)\n",
      "\n",
      "Epoch 786 - Before training loss: 364.5910949707031 | After training loss: 410.12188720703125\n",
      "Epoch 787 - Before training loss: 410.12188720703125 | After training loss: 370.79754638671875\n",
      "Epoch 788 - Before training loss: 370.79754638671875 | After training loss: 403.27777099609375\n",
      "Epoch 789 - Before training loss: 403.27777099609375 | After training loss: 388.58221435546875\n",
      "Epoch 790 - Before training loss: 388.58221435546875 | After training loss: 449.9960021972656\n",
      "\n",
      "Test set: Average loss: 7.8474, Accuracy: 3017/10000 (30.17%)\n",
      "\n",
      "Epoch 791 - Before training loss: 391.93084716796875 | After training loss: 384.1487121582031\n",
      "Epoch 792 - Before training loss: 384.1487121582031 | After training loss: 436.3702392578125\n",
      "Epoch 793 - Before training loss: 436.3702392578125 | After training loss: 421.3170166015625\n",
      "Epoch 794 - Before training loss: 421.3170166015625 | After training loss: 460.2066955566406\n",
      "Epoch 795 - Before training loss: 460.2066955566406 | After training loss: 451.1786193847656\n",
      "\n",
      "Test set: Average loss: 7.3653, Accuracy: 2943/10000 (29.43%)\n",
      "\n",
      "Epoch 796 - Before training loss: 408.86932373046875 | After training loss: 504.16455078125\n",
      "Epoch 797 - Before training loss: 504.16455078125 | After training loss: 454.21490478515625\n",
      "Epoch 798 - Before training loss: 454.21490478515625 | After training loss: 467.6130065917969\n",
      "Epoch 799 - Before training loss: 467.6130065917969 | After training loss: 492.83135986328125\n",
      "Loss: MSE | Optimizer: SGD | Lr: 0.1\n"
     ]
    }
   ],
   "source": [
    "# BEST: Loss: MSE | Optimizer: SGD | Lr: 0.1 - Final / Initial loss %: 66.11437225341797\n",
    "\n",
    "epochs = 800\n",
    "fc_epochs = 5\n",
    "\n",
    "print(file_path)\n",
    "\n",
    "for loss_fct, loss_name in zip([F.mse_loss], ['MSE']):\n",
    "    for optim, optim_name in zip([torch.optim.SGD],['SGD']):\n",
    "        for lr in [0.1]:\n",
    "            net = Net()\n",
    "            net.load_state_dict(torch.load('net_kornblith/net_kornblith_all_1.zip'))\n",
    "            net.cuda()\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))\n",
    "            for i in range(epochs):\n",
    "                a, b = train_embeds(net, device, train_loaderx, loss_fct = loss_fct, optimizer = optim, lr = lr, epoch = i, display=True)\n",
    "#                 if i == 0: initial_loss = a\n",
    "#                 if i == 499: final_loss = b\n",
    "                if i%fc_epochs==0:\n",
    "                    for j in [0,1,2,3,4]:\n",
    "                        train_fc_layer(net, device, train_loaderx, loss_fct = F.cross_entropy, optimizer = optim, lr = 0.01,  display=False)\n",
    "                    test(net, torch.device('cuda'), test_loaderx)\n",
    "                    \n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))#+' - Final / Initial loss %: {}'.format(final_loss/initial_loss*100))\n",
    "    \n",
    "torch.save(net.state_dict(), 'net_kornblith/net_kall1_split_train_fc{}_lr0.01'.format(fc_epochs)+experiment+'_mse_sgd_lr0.1_{}e.t7'.format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Test set: Average loss: 0.4737, Accuracy: 8509/10000 (85.09%)\n",
      "\n",
      "New:\n",
      "\n",
      "Test set: Average loss: 12.2197, Accuracy: 1627/10000 (16.27%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.27"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('net_kornblith/net_kornblith_all_1.zip'))\n",
    "net.cuda()\n",
    "print('Original:')\n",
    "test(net, torch.device('cuda'), test_loaderx)\n",
    "\n",
    "net2 = Net()\n",
    "net2.load_state_dict(torch.load('net_kornblith/net_kall1_split_train_mse_sgd_lr0.1_500e.t7'))\n",
    "net2.cuda()\n",
    "print('New:')\n",
    "test(net2, torch.device('cuda'), test_loaderx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cifar10_sorted_split_last_layer_embeds_2num-clusters_1000dist-clusters_num_solve_10000pts-cka_0seed.pt\n",
      "Loss: Cross Entropy | Optimizer: ADAM | Lr: 0.1\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 50.418262\n",
      "\n",
      "Test set: Average loss: 23.1332, Accuracy: 2082/10000 (20.82%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 48.817303\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 48.413574\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 52.935688\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 31.923830\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.065235\n",
      "\n",
      "Test set: Average loss: 26.2372, Accuracy: 2328/10000 (23.28%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.430296\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 29.400703\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.755253\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 30.038809\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.374107\n",
      "\n",
      "Test set: Average loss: 20.6118, Accuracy: 2768/10000 (27.68%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 35.862785\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 35.642010\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 34.430157\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 33.167931\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 34.364792\n",
      "\n",
      "Test set: Average loss: 14.0224, Accuracy: 2614/10000 (26.14%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 25.288111\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.889439\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 43.238091\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.216621\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 29.915073\n",
      "\n",
      "Test set: Average loss: 13.6733, Accuracy: 2633/10000 (26.33%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 28.151632\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 31.037161\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 25.411751\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.720602\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 48.337692\n",
      "\n",
      "Test set: Average loss: 26.1334, Accuracy: 2770/10000 (27.70%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 28.901558\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 34.022369\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 58.183414\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 11.813132\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 35.796188\n",
      "\n",
      "Test set: Average loss: 20.4298, Accuracy: 2534/10000 (25.34%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 42.163353\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.892605\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 37.999138\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 40.150307\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.258379\n",
      "\n",
      "Test set: Average loss: 10.6407, Accuracy: 2658/10000 (26.58%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 14.164724\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 22.191370\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.428429\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 27.404167\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.676655\n",
      "\n",
      "Test set: Average loss: 12.2037, Accuracy: 2876/10000 (28.76%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 27.418091\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 25.837992\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.622271\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 44.069145\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 30.610487\n",
      "\n",
      "Test set: Average loss: 20.3435, Accuracy: 2816/10000 (28.16%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 54.546719\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 39.301533\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 40.086784\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 23.913101\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 33.239704\n",
      "\n",
      "Test set: Average loss: 24.5480, Accuracy: 2850/10000 (28.50%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 35.415276\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 40.710808\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 28.898130\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 21.180828\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.368923\n",
      "\n",
      "Test set: Average loss: 14.3121, Accuracy: 2834/10000 (28.34%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 18.718708\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.193619\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.771706\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 21.851954\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 23.161789\n",
      "\n",
      "Test set: Average loss: 15.3843, Accuracy: 2805/10000 (28.05%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 28.518425\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 40.555977\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.675596\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 16.787994\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 25.745005\n",
      "\n",
      "Test set: Average loss: 16.9591, Accuracy: 2589/10000 (25.89%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 33.812805\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.463615\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 31.474167\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 19.627926\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 35.301197\n",
      "\n",
      "Test set: Average loss: 22.7516, Accuracy: 2960/10000 (29.60%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 27.257017\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.881107\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.897951\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 23.886061\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 27.743599\n",
      "\n",
      "Test set: Average loss: 17.3845, Accuracy: 2679/10000 (26.79%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.913971\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.673191\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 22.558208\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 27.507992\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 22.504717\n",
      "\n",
      "Test set: Average loss: 19.4772, Accuracy: 2732/10000 (27.32%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 22.913486\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 23.169519\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 14.006475\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 36.568596\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 26.796955\n",
      "\n",
      "Test set: Average loss: 16.2911, Accuracy: 2827/10000 (28.27%)\n",
      "\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 23.377707\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 24.414890\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 16.732334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-337d103ec028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' | Optimizer: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moptim_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' | Lr: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mtrain_fc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loaderx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#                 if i == 0: initial_loss = a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#                 if i == 499: final_loss = b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-cd3eb1e721a2>\u001b[0m in \u001b[0;36mtrain_fc_layer\u001b[0;34m(model, device, train_loader, loss_fct, optimizer, lr, display)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_embed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-2e735c299557>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/main_env/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BEST: Loss: MSE | Optimizer: SGD | Lr: 0.1 - Final / Initial loss %: 66.11437225341797\n",
    "\n",
    "epochs = 500\n",
    "#from 800\n",
    "#100 epochs didn't help\n",
    "# 100 more epochs 0.1 didn't help much (error 25%)\n",
    "# Maybe need smaller lr? loss bounces around a lot\n",
    "# 100 more epochs 0.01\n",
    "## Significant improvement (35% to 30%) in the first few epochs\n",
    "## then seems to have plateau'd\n",
    "# 100 more epochs 0.01\n",
    "\n",
    "print(file_path)\n",
    "\n",
    "for loss_fct, loss_name in zip([F.cross_entropy], ['Cross Entropy']):\n",
    "    for optim, optim_name in zip([torch.optim.Adam],['ADAM']):\n",
    "        for lr in [0.1]:\n",
    "#             net = Net()\n",
    "#             net.load_state_dict(torch.load('net_kornblith/net_kall1_split_train_fc5_mse_sgd_lr0.1_800e.t7'))\n",
    "#             net.cuda()\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))\n",
    "            for i in range(epochs):\n",
    "                train_fc_layer(net, device, train_loaderx, loss_fct = loss_fct, optimizer = optim, lr = lr,  display=True)\n",
    "#                 if i == 0: initial_loss = a\n",
    "#                 if i == 499: final_loss = b\n",
    "                if i%5 == 0: test(net, torch.device('cuda'), test_loaderx)\n",
    "            print('Loss: '+loss_name+' | Optimizer: '+optim_name+' | Lr: {}'.format(lr))#+' - Final / Initial loss %: {}'.format(final_loss/initial_loss*100))\n",
    "    \n",
    "# torch.save(net.state_dict(), 'net_kornblith/net_kall1_split_train'+experiment+'_mse_sgd_lr0.1_{}e.t7'.format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple training tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,x,y,z in train_loaderx:\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function mse_loss at 0x7f6ee04f7440>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [20352/50000 (96%)]\tLoss: 94.504486\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 30.571859\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 68.288193\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 70.667366\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 64.377548\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 56.695179\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 62.306759\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 66.344521\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 60.358624\n",
      "Train Epoch: [20352/50000 (96%)]\tLoss: 38.516537\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_fc_layer(net_all1, device, train_loaderx, lr = 1, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight\n",
      "layers.0.0.bias\n",
      "layers.0.1.weight\n",
      "layers.0.1.bias\n",
      "layers.1.0.weight\n",
      "layers.1.0.bias\n",
      "layers.1.1.weight\n",
      "layers.1.1.bias\n",
      "layers.2.0.weight\n",
      "layers.2.0.bias\n",
      "layers.2.1.weight\n",
      "layers.2.1.bias\n",
      "layers.3.0.weight\n",
      "layers.3.0.bias\n",
      "layers.3.1.weight\n",
      "layers.3.1.bias\n",
      "layers.4.0.weight\n",
      "layers.4.0.bias\n",
      "layers.4.1.weight\n",
      "layers.4.1.bias\n",
      "layers.5.0.weight\n",
      "layers.5.0.bias\n",
      "layers.5.1.weight\n",
      "layers.5.1.bias\n",
      "layers.6.0.weight\n",
      "layers.6.0.bias\n",
      "layers.6.1.weight\n",
      "layers.6.1.bias\n",
      "layers.7.0.weight\n",
      "layers.7.0.bias\n",
      "layers.7.1.weight\n",
      "layers.7.1.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in net_all1.named_parameters():\n",
    "    print(name)\n",
    "#     print(param.requires_grad)\n",
    "    if \"layers\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight\n",
      "False\n",
      "layers.0.0.bias\n",
      "False\n",
      "layers.0.1.weight\n",
      "False\n",
      "layers.0.1.bias\n",
      "False\n",
      "layers.1.0.weight\n",
      "False\n",
      "layers.1.0.bias\n",
      "False\n",
      "layers.1.1.weight\n",
      "False\n",
      "layers.1.1.bias\n",
      "False\n",
      "layers.2.0.weight\n",
      "False\n",
      "layers.2.0.bias\n",
      "False\n",
      "layers.2.1.weight\n",
      "False\n",
      "layers.2.1.bias\n",
      "False\n",
      "layers.3.0.weight\n",
      "False\n",
      "layers.3.0.bias\n",
      "False\n",
      "layers.3.1.weight\n",
      "False\n",
      "layers.3.1.bias\n",
      "False\n",
      "layers.4.0.weight\n",
      "False\n",
      "layers.4.0.bias\n",
      "False\n",
      "layers.4.1.weight\n",
      "False\n",
      "layers.4.1.bias\n",
      "False\n",
      "layers.5.0.weight\n",
      "False\n",
      "layers.5.0.bias\n",
      "False\n",
      "layers.5.1.weight\n",
      "False\n",
      "layers.5.1.bias\n",
      "False\n",
      "layers.6.0.weight\n",
      "False\n",
      "layers.6.0.bias\n",
      "False\n",
      "layers.6.1.weight\n",
      "False\n",
      "layers.6.1.bias\n",
      "False\n",
      "layers.7.0.weight\n",
      "False\n",
      "layers.7.0.bias\n",
      "False\n",
      "layers.7.1.weight\n",
      "False\n",
      "layers.7.1.bias\n",
      "False\n",
      "fc.weight\n",
      "True\n",
      "fc.bias\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, param in net_all1.named_parameters():\n",
    "    print(name)\n",
    "    print(param.requires_grad)\n",
    "#     if \"layers\" in name:\n",
    "#         param.requires_grad = False\n",
    "#     else:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "dist_clusters = 1000\n",
    "splitting_dir = 'pc5'\n",
    "num_pts_cka = 10000\n",
    "\n",
    "data_per_classes, indexes = get_activations_by_class(train_act, train_labels)\n",
    "lin_sep, cka, split_embeds, indexes, split_indexes = test_cka_lin_sep(data_per_classes, indexes, lin_svc, num_clusters = 2, dist_clusters = 10000, splitting_dir = 'pc5', num_pts_cka = 10000, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a linear SVM classifier on the original data: 0.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "train_activations, _ = net_all1.forward(train_data, all_act=True)\n",
    "train_act = train_activations[-1].reshape(train_activations[-1].shape[0],-1).detach().cpu().numpy()\n",
    "train_act_tensors = train_activations[-1]\n",
    "for i in train_activations[:-1]: del i\n",
    "del train_activations\n",
    "\n",
    "# Linear separability:\n",
    "lin_svc = LinearSVC()\n",
    "lin_svc.fit(train_act, train_labels)\n",
    "original_lin_sep = lin_svc.score(train_act, train_labels)\n",
    "print(\"Accuracy of a linear SVM classifier on the original data: {}\".format(original_lin_sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check that the split does what it's supposed to and the indexes saved are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2; Distance between clusters: 10000; Splitting direction: pc5; Number of points to compute CKA: 10000\n",
      "Accuracy of the linear SVM classifier on the split data: 0.9129\n",
      "Cka between 10000 original vs split pts: 0.3227207064628601\n"
     ]
    }
   ],
   "source": [
    "data_per_classes, indexes = get_activations_by_class(train_act, train_labels)\n",
    "lin_sep, cka, split_embeds, indexes, split_indexes = test_cka_lin_sep(data_per_classes, indexes, lin_svc, num_clusters = 2, dist_clusters = 10000, splitting_dir = 'pc5', num_pts_cka = 10000, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-af89278b7b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     for j in i:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#         print(len(j))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#should be equl to 5000 (examples per class separated into two clusters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlisst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_indexes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in split_indexes:\n",
    "#     for j in i:\n",
    "#         print(len(j))\n",
    "    print(len(i[0])+len(i[1])) #should be equl to 5000 (examples per class separated into two clusters)\n",
    "    \n",
    "lisst = [k for i in split_indexes for j in i for k in j]\n",
    "print(len(set(lisst))) # should be 50000\n",
    "lisst.sort\n",
    "print(sum(np.arange(50000)-np.array(lisst))) # should be equal to 0 (i.e. contains every element from 0 to 49999 included and only these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "idx = 7\n",
    "print(indexes[12])\n",
    "print(split_indexes[0][0][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 64, 2, 2])\n",
      "torch.Size([50000, 64, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(train_act_tensors.shape)\n",
    "reshaped = train_act_tensors.reshape(train_act_tensors.shape[0],-1).reshape([50000,64,2,2])\n",
    "print(reshaped.shape)\n",
    "# print(train_act_tensors[0])\n",
    "# print(reshaped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "reshaped = train_act_tensors.reshape(train_act_tensors.shape[0],-1)\n",
    "reshaped = reshaped.reshape([reshaped.shape[0],64,2,2])\n",
    "\n",
    "differences = []\n",
    "differences1 = []\n",
    "for idx, i in enumerate(train_act_tensors):\n",
    "    differences.append(np.sum(i.detach().cpu().numpy()-reshaped[idx].detach().cpu().numpy()))\n",
    "    differences1.append(np.sum(i.detach().cpu().numpy()-i.reshape(256).reshape([64,2,2]).detach().cpu().numpy()))\n",
    "print(sum(differences))\n",
    "print(sum(differences1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape works in both directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the index and the new points in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]) \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "##### Cifar Data\n",
    "cifar_data = datasets.CIFAR10(root='.',train=True, transform=transform_train, download=True)\n",
    "cifar_data_test = datasets.CIFAR10(root='.',train=False, transform=transform_val, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Tuple\n",
    "class CIFAR10_mine(datasets.CIFAR10):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = True,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            download: bool = False) -> None:\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        a = super().__getitem__(index)\n",
    "        return (a[0], a[1], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) \n",
    "\n",
    "cifar_data = CIFAR10_mine(root='.', train=True, download=True, transform = transform_val)\n",
    "train_loader = torch.utils.data.DataLoader(cifar_data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,z in train_loader:\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr(lin_svc.coef_.T)\n",
    "Q = Q.T\n",
    "\n",
    "direction = np.random.normal(0, 1, 256).T\n",
    "norm = np.sum(direction**2)**(0.5)\n",
    "direction = direction/norm\n",
    "direction = np.absolute(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quadprog\n",
    "\n",
    "def quadprog_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    qp_G = .5 * (P + P.T)   # make sure P is symmetric\n",
    "    qp_a = -q\n",
    "    if A is not None:\n",
    "        qp_C = -numpy.vstack([A, G]).T\n",
    "        qp_b = -numpy.hstack([b, h])\n",
    "        meq = A.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = -G.T\n",
    "        qp_b = -h\n",
    "        meq = 0\n",
    "    return quadprog.solve_qp(qp_G, qp_a, qp_C, qp_b, meq)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Q\n",
    "# P = np.dot(M.T, M) # Not positive definite\n",
    "epsilon = 0.00000001\n",
    "P = np.dot(M.T, M) + epsilon*np.eye(M.shape[1]) # Adding epsilon * identity to make it positive definite\n",
    "q = -np.dot(M.T, np.zeros(10))\n",
    "G = -np.eye(256)\n",
    "# h = np.zeros(256) # returns all zeros\n",
    "h =  -np.ones(256)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1        0.1        0.1        0.1        0.1        0.1395443\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.35295926 0.74807263 0.37181194 0.36263526\n",
      " 0.66555696 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.14020782\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.34627948 0.53691978\n",
      " 0.66459626 0.53667065 0.37368035 0.8263061  0.31106512 0.1\n",
      " 0.1        0.38998344 0.26835046 0.24798285 0.1        0.1\n",
      " 0.1        0.1        0.25463257 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.17849729 0.1        0.1\n",
      " 0.1        0.1        0.15660197 0.1        0.1        0.1\n",
      " 0.1        0.1        0.47445683 0.24342873 0.1        0.1\n",
      " 0.1        0.1        0.49283448 0.1        0.1        0.1\n",
      " 0.28920587 0.23837759 1.25208227 1.57400654 0.1        0.1\n",
      " 0.1        0.1        0.1587269  0.1        0.1        0.21494529\n",
      " 0.1        0.1        0.1        0.1        0.63647954 0.1\n",
      " 0.1        0.18427864 0.1        0.1        0.1        0.1\n",
      " 0.1        0.1160944  0.65459984 0.36784013 0.1        0.1\n",
      " 0.16153532 0.34702122 0.1        0.1        0.1        0.1\n",
      " 0.1        0.66725562 0.26126276 0.1        0.1        0.1\n",
      " 0.1        0.1        0.38411245 0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.12177971 0.1        0.2630327  0.11056885\n",
      " 0.50729501 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.44318546 0.278665   0.1        0.52767191 0.36897107\n",
      " 0.11282302 0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.1        0.1        0.1        0.1        0.1        0.1\n",
      " 0.78517833 0.55871335 0.1        1.05139368 0.1        0.1\n",
      " 0.1        0.1        0.32597739 0.1        0.1        0.1\n",
      " 0.26337694 0.37379287 0.54797824 0.77113427 0.1        0.1\n",
      " 0.1        0.1        0.1818112  0.1        0.16190945 0.49675046\n",
      " 0.1        0.1        0.25645852 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "sol = quadprog_solve_qp(P, q, G, h)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.41384983e-08,  4.41450581e-08,  4.58886932e-08,  5.13632785e-08,\n",
       "        3.53498133e-08,  4.62222439e-08,  4.05178575e-08, -3.73668767e-08,\n",
       "       -4.34134963e-08, -3.51095555e-08])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(M, sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\n",
    "\n",
    "solutions = []\n",
    "values = []\n",
    "val_norms = []\n",
    "\n",
    "for index, epsilon in enumerate(epsilons):\n",
    "    M = Q\n",
    "    P = np.dot(M.T, M) + epsilon*np.eye(M.shape[1]) # Adding epsilon * identity to make it positive definite\n",
    "    q = -np.dot(M.T, np.zeros(10))\n",
    "    G = -np.eye(256)\n",
    "    h =  -np.ones(256)*0.1\n",
    "\n",
    "    solutions.append(quadprog_solve_qp(P, q, G, h))\n",
    "    values.append(np.dot(M, solutions[-1]))\n",
    "    val_norms.append(np.linalg.norm(values[-1]))\n",
    "\n",
    "sol_norms = [np.linalg.norm(solutions[i]-solutions[-1]) for i in range(len(solutions))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons, sol_norms)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "# Almost linear relation betwen value of epsilon and how far the solution is from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons, sol_norms)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sd' in 'fdsasd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Basic_CKA_ForSharing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (main_env)",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "522e0f024813495bb45905626fdccd14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8de220001a4c42e6b2bf369d63e17658",
       "IPY_MODEL_9a5b5204bd3847d485f8a139949a79f4",
       "IPY_MODEL_b0283df5d40143789293e11c9428d543"
      ],
      "layout": "IPY_MODEL_916a6726fe5b4b8a949d1d6b1d4bfec1"
     }
    },
    "6d746b9790c744ac91ff6c7cc852f9f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70d14a62a0e94362bea840e44b5719e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "890787b726f4485199dcb2a059ef92a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8de220001a4c42e6b2bf369d63e17658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d746b9790c744ac91ff6c7cc852f9f7",
      "placeholder": "",
      "style": "IPY_MODEL_ad439d60a52447469bcdce66a8e3c0bd",
      "value": ""
     }
    },
    "8ec5fa1d3675438ba3e5097f6f605325": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "916a6726fe5b4b8a949d1d6b1d4bfec1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a5b5204bd3847d485f8a139949a79f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e34af8bd1fe3408cbd99341d4271f845",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_890787b726f4485199dcb2a059ef92a3",
      "value": 170498071
     }
    },
    "ad439d60a52447469bcdce66a8e3c0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0283df5d40143789293e11c9428d543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70d14a62a0e94362bea840e44b5719e3",
      "placeholder": "",
      "style": "IPY_MODEL_8ec5fa1d3675438ba3e5097f6f605325",
      "value": " 170499072/? [00:05&lt;00:00, 32259211.76it/s]"
     }
    },
    "e34af8bd1fe3408cbd99341d4271f845": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
